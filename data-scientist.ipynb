{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "799d20ed-fc04-462e-960c-6f6e1f6b1a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ad9acd-128a-4ce5-bbbb-c75ad7b45529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools.base import BaseToolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea135774-1539-4b0a-8ebb-1f600eb828fa",
   "metadata": {},
   "source": [
    "Decompose the Problem: CoT prompts guide the model to break down a complex question into manageable steps, akin to how a human might solve the problem.\n",
    "Guide with Exemplars: CoT uses examples that demonstrate reasoning steps, helping the model grasp the method needed to reach the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e0ae4-97be-4caf-a82c-feb5b231bc29",
   "metadata": {},
   "source": [
    "Zero-Shot CoT and CoT Prompting both aim to improve model responses and extract more accurate answers by generating logic-based reasoning. In Zero-Shot CoT, however, we do not have to include input exemplars of Chain-of-Thought responses, but rather just append the words \"Let's think step by step\" to the end of an input.\n",
    "\n",
    "Example:\n",
    "\n",
    "> If John has 5 pears, then eats 2, and buys 5 more, then gives 3 to his friend, how many pears does he have?\n",
    "> \n",
    "> Let's think step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff7c2da-14e1-4831-9ac3-6ff368f69418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import TypedDict, List, Optional, Dict, Any\n",
    "from sklearn.impute import SimpleImputer\n",
    "from langgraph.graph import StateGraph, END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d1f3a33-bd36-49ee-b4b3-c9782495f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Define Agent State ---\n",
    "\n",
    "class CleaningState(TypedDict):\n",
    "    \"\"\"Represents the state of our data cleaning pipeline.\"\"\"\n",
    "    file_path: str               # Input CSV file path\n",
    "    dataframe: Optional[pd.DataFrame] # The data being processed\n",
    "    report: List[str]            # Log of actions performed\n",
    "    null_threshold: float        # % threshold to drop columns\n",
    "    imputation_results: Optional[Dict[str, Any]] # Store imputer details\n",
    "\n",
    "# --- 2. Define Cleaning Tool Functions (as Nodes) ---\n",
    "\n",
    "def load_data(state: CleaningState) -> Dict[str, Any]:\n",
    "    \"\"\"Loads the initial data from the CSV file.\"\"\"\n",
    "    print(\"--- Node: load_data ---\")\n",
    "    file_path = state['file_path']\n",
    "    report = state.get('report', []) # Initialize report if not present\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded data. Shape: {df.shape}\")\n",
    "        report.append(f\"Loaded data from '{file_path}'. Initial shape: {df.shape}\")\n",
    "        return {\"dataframe\": df, \"report\": report}\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        report.append(f\"Error: File not found at {file_path}. Stopping execution.\")\n",
    "        # Returning None for dataframe signals an issue, could also raise an exception\n",
    "        # or add an error flag to the state if graph needs more complex error handling.\n",
    "        return {\"dataframe\": None, \"report\": report}\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV: {e}\")\n",
    "        report.append(f\"Error loading CSV: {e}. Stopping execution.\")\n",
    "        return {\"dataframe\": None, \"report\": report}\n",
    "\n",
    "def drop_high_null_columns(state: CleaningState) -> Dict[str, Any]:\n",
    "    \"\"\"Drops columns exceeding the null value threshold.\"\"\"\n",
    "    print(\"--- Node: drop_high_null_columns ---\")\n",
    "    df = state['dataframe']\n",
    "    report = state['report']\n",
    "    threshold = state['null_threshold']\n",
    "\n",
    "    if df is None:\n",
    "        report.append(\"Skipping drop_high_null_columns: DataFrame not loaded.\")\n",
    "        print(\"Skipping: DataFrame not loaded.\")\n",
    "        return {\"report\": report} # No change to dataframe\n",
    "\n",
    "    initial_cols = df.shape[1]\n",
    "    null_percentages = df.isnull().mean()\n",
    "    cols_to_drop = null_percentages[null_percentages > threshold].index.tolist()\n",
    "\n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        dropped_cols_str = \", \".join(cols_to_drop)\n",
    "        report.append(f\"Dropped columns with >{threshold*100}% nulls: [{dropped_cols_str}].\")\n",
    "        print(f\"Dropped columns: {cols_to_drop}\")\n",
    "        print(f\"New shape: {df.shape}\")\n",
    "    else:\n",
    "        report.append(f\"No columns found with >{threshold*100}% null values.\")\n",
    "        print(\"No columns to drop based on threshold.\")\n",
    "\n",
    "    return {\"dataframe\": df, \"report\": report}\n",
    "\n",
    "def drop_all_null_rows(state: CleaningState) -> Dict[str, Any]:\n",
    "    \"\"\"Drops rows where all values are null.\"\"\"\n",
    "    print(\"--- Node: drop_all_null_rows ---\")\n",
    "    df = state['dataframe']\n",
    "    report = state['report']\n",
    "\n",
    "    if df is None:\n",
    "        report.append(\"Skipping drop_all_null_rows: DataFrame not available.\")\n",
    "        print(\"Skipping: DataFrame not available.\")\n",
    "        return {\"report\": report}\n",
    "\n",
    "    initial_rows = df.shape[0]\n",
    "    df = df.dropna(how='all')\n",
    "    rows_dropped = initial_rows - df.shape[0]\n",
    "\n",
    "    if rows_dropped > 0:\n",
    "        report.append(f\"Dropped {rows_dropped} rows containing all null values.\")\n",
    "        print(f\"Dropped {rows_dropped} all-null rows.\")\n",
    "        print(f\"New shape: {df.shape}\")\n",
    "    else:\n",
    "        report.append(\"No rows found containing all null values.\")\n",
    "        print(\"No all-null rows to drop.\")\n",
    "\n",
    "    return {\"dataframe\": df, \"report\": report}\n",
    "\n",
    "\n",
    "def impute_missing_values(state: CleaningState) -> Dict[str, Any]:\n",
    "    \"\"\"Imputes missing values (NaN and empty strings) based on column type.\"\"\"\n",
    "    print(\"--- Node: impute_missing_values ---\")\n",
    "    df = state['dataframe']\n",
    "    report = state['report']\n",
    "    imputation_summary = {} # Store details about imputation\n",
    "\n",
    "    if df is None:\n",
    "        report.append(\"Skipping impute_missing_values: DataFrame not available.\")\n",
    "        print(\"Skipping: DataFrame not available.\")\n",
    "        return {\"report\": report}\n",
    "\n",
    "    # Replace empty strings with NaN to be caught by imputation\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    report.append(\"Replaced empty strings with NaN for imputation.\")\n",
    "    print(\"Replaced empty strings with NaN.\")\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include='object').columns.tolist() # Simple typing\n",
    "\n",
    "    imputed_cols_count = 0\n",
    "\n",
    "    # Impute Numeric Columns (using median)\n",
    "    if numeric_cols:\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "        # Only impute columns that actually have NaNs\n",
    "        numeric_cols_with_nan = df[numeric_cols].isnull().any()\n",
    "        cols_to_impute_num = numeric_cols_with_nan[numeric_cols_with_nan].index.tolist()\n",
    "        if cols_to_impute_num:\n",
    "            print(f\"Imputing numeric columns (median): {cols_to_impute_num}\")\n",
    "            df[cols_to_impute_num] = num_imputer.fit_transform(df[cols_to_impute_num])\n",
    "            imputation_summary['numeric_median'] = cols_to_impute_num\n",
    "            imputed_cols_count += len(cols_to_impute_num)\n",
    "        else:\n",
    "            print(\"No numeric columns require imputation.\")\n",
    "\n",
    "\n",
    "    # Impute Categorical Columns (using most frequent)\n",
    "    if categorical_cols:\n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "         # Only impute columns that actually have NaNs\n",
    "        categorical_cols_with_nan = df[categorical_cols].isnull().any()\n",
    "        cols_to_impute_cat = categorical_cols_with_nan[categorical_cols_with_nan].index.tolist()\n",
    "        if cols_to_impute_cat:\n",
    "            print(f\"Imputing categorical columns (most_frequent): {cols_to_impute_cat}\")\n",
    "            df[cols_to_impute_cat] = cat_imputer.fit_transform(df[cols_to_impute_cat])\n",
    "            imputation_summary['categorical_most_frequent'] = cols_to_impute_cat\n",
    "            imputed_cols_count += len(cols_to_impute_cat)\n",
    "        else:\n",
    "            print(\"No categorical columns require imputation.\")\n",
    "\n",
    "    if imputed_cols_count > 0:\n",
    "        report.append(f\"Imputed missing values in {imputed_cols_count} column(s). Details: {imputation_summary}\")\n",
    "    else:\n",
    "        report.append(\"No missing values found requiring imputation.\")\n",
    "\n",
    "    return {\"dataframe\": df, \"report\": report, \"imputation_results\": imputation_summary}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656b559d-b208-4548-8289-6bcea7003da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(CleaningState)\n",
    "\n",
    "workflow.add_node(\"load\", load_data)\n",
    "workflow.add_node(\"drop_cols\", drop_high_null_columns)\n",
    "workflow.add_node(\"drop_rows\", drop_all_null_rows)\n",
    "workflow.add_node(\"impute\", impute_missing_values)\n",
    "\n",
    "# Define the sequence of execution\n",
    "workflow.set_entry_point(\"load\")\n",
    "workflow.add_edge(\"load\", \"drop_cols\")\n",
    "workflow.add_edge(\"drop_cols\", \"drop_rows\")\n",
    "workflow.add_edge(\"drop_rows\", \"impute\")\n",
    "workflow.add_edge(\"impute\", END)\n",
    "\n",
    "# Compile the graph\n",
    "cleaning_agent = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1614ca-8321-4d05-9af6-0576508d7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5, 6, 7],\n",
    "    'Name': ['Alice', 'Bob', '', 'David', 'Eve', 'Frank', 'Grace'],\n",
    "    'Age': [25, 30, 35, None, 28, 45, 50],\n",
    "    'City': ['New York', 'London', 'Paris', 'London', '', 'Sydney', None],\n",
    "    'Salary': [50000, 60000, 75000, None, 55000, None, 90000],\n",
    "    'HighNullCol': [None, None, None, None, None, 'Value', None], # >70% null\n",
    "    'AllNullRowIndicator': [None, None, None, None, None, None, None] # Used to create an all-null row later\n",
    "}\n",
    "sample_df = pd.DataFrame(data)\n",
    "all_null_row = pd.Series([None] * len(sample_df.columns), index=sample_df.columns)\n",
    "sample_df = pd.concat([sample_df, pd.DataFrame([all_null_row])], ignore_index=True)\n",
    "\n",
    "\n",
    "sample_df['AllNullRowIndicator'] = None\n",
    "\n",
    "\n",
    "csv_file_path = \"sample_data_to_clean.csv\"\n",
    "sample_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Created sample CSV: {csv_file_path}\")\n",
    "print(\"--- Original Data ---\")\n",
    "print(sample_df)\n",
    "print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f607bdf9-df23-4a82-bdff-c57e33c63849",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state: CleaningState = {\n",
    "    \"file_path\": csv_file_path,\n",
    "    \"dataframe\": None,\n",
    "    \"report\": [],\n",
    "    \"null_threshold\": 0.7,\n",
    "    \"imputation_results\": None\n",
    "}\n",
    "\n",
    "print(\"\\n--- Starting Cleaning Agent ---\")\n",
    "final_state = cleaning_agent.invoke(initial_state)\n",
    "print(\"\\n--- Cleaning Agent Finished ---\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Final Report ---\")\n",
    "for msg in final_state['report']:\n",
    "    print(f\"- {msg}\")\n",
    "\n",
    "print(\"\\n--- Final Cleaned Data ---\")\n",
    "if final_state['dataframe'] is not None:\n",
    "    print(final_state['dataframe'])\n",
    "    final_state['dataframe'].to_csv(\"sample_data_to_clean_cleaned.csv\")\n",
    "    print(f\"\\nFinal Shape: {final_state['dataframe'].shape}\")\n",
    "    print(\"\\nNull values after cleaning:\")\n",
    "    print(final_state['dataframe'].isnull().sum())\n",
    "else:\n",
    "    print(\"Data cleaning could not be completed due to errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae31df8e-1679-4f05-b46d-7bf0e8149b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab871e9-8a49-4daa-bbe0-c3e0ec7ba167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cca2a3-d549-40b2-955e-2148b0ae52ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff327e2-01c1-467d-9c26-77b0564fb1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import TypedDict, Annotated, Sequence, Optional, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.pregel import RunnableConfig\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb2b64c5-4de9-4505-bda8-4871ee15b5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "LLAMA_8B = \"llama3-8b-8192\"\n",
    "LLAMA_70B = \"llama3-70b-8192\"\n",
    "GEMMA2_9B = \"gemma2-9b-it\"\n",
    "\n",
    "model = ChatGroq(model=LLAMA_8B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3f174bf-0398-4662-9599-97113f53e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a3cec1f-a775-489b-adce-0e2c423648d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_summary(df: dict) -> str:\n",
    "    \"\"\"Generates a concise summary of the DataFrame for the LLM.\"\"\"\n",
    "    if df is None:\n",
    "        return \"No data loaded.\"\n",
    "\n",
    "    df = pd.DataFrame.from_dict(df)\n",
    "    summary = []\n",
    "    summary.append(f\"DataFrame Shape: {df.shape}\")\n",
    "    null_counts = df.isnull().sum()\n",
    "    empty_string_counts = (df == '').sum()\n",
    "    total_missing = null_counts + empty_string_counts\n",
    "    missing_summary = total_missing[total_missing > 0].sort_values(ascending=False)\n",
    "\n",
    "    if not missing_summary.empty:\n",
    "        summary.append(\"\\nMissing Values (NaN or ''):\")\n",
    "        for col, count in missing_summary.items():\n",
    "            percent = (count / df.shape[0]) * 100\n",
    "            summary.append(f\"- {col}: {count} ({percent:.2f}%)\")\n",
    "    else:\n",
    "        summary.append(\"\\nNo missing values (NaN or '') found.\")\n",
    "\n",
    "    summary.append(\"\\nColumn Data Types:\")\n",
    "    summary.append(df.dtypes.to_string())\n",
    "\n",
    "    # Add preview of first few rows for context\n",
    "    summary.append(\"\\nData Preview (first 3 rows):\")\n",
    "    summary.append(df.head(3).to_string())\n",
    "\n",
    "    return \"\\n\".join(summary)\n",
    "\n",
    "# --- 2. Define Cleaning Tools ---\n",
    "\n",
    "def drop_high_null_columns(df: dict, null_threshold_percent: float = 50.0) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Drops columns from the DataFrame where the percentage of missing values\n",
    "    (NaN or empty string) exceeds the specified threshold.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        null_threshold_percent (float): The percentage threshold (0-100). Default is 50.\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing the modified DataFrame under the key 'dataframe'\n",
    "                        and a status message under the key 'status'.\n",
    "    \"\"\"\n",
    "    print(f\"--- Calling drop_high_null_columns (threshold: {null_threshold_percent}%) ---\")\n",
    "    if df is None:\n",
    "         return {\"dataframe\": None, \"status\": \"Error: No DataFrame provided.\"}\n",
    "\n",
    "    df = pd.DataFrame.from_dict(df)\n",
    "    # Consider both NaN and empty strings as missing\n",
    "    df_copy = df.replace('', np.nan)\n",
    "    initial_cols = set(df_copy.columns)\n",
    "    threshold = null_threshold_percent / 100.0\n",
    "    cols_to_drop = []\n",
    "\n",
    "    for col in df_copy.columns:\n",
    "        null_ratio = df_copy[col].isnull().sum() / len(df_copy)\n",
    "        if null_ratio > threshold:\n",
    "            cols_to_drop.append(col)\n",
    "\n",
    "    if not cols_to_drop:\n",
    "        status = f\"No columns found with missing values > {null_threshold_percent}%.\"\n",
    "        print(f\"--- Status: {status} ---\")\n",
    "        return {\"dataframe\": df, \"status\": status} # Return original df if no changes\n",
    "    else:\n",
    "        df_cleaned = df.drop(columns=cols_to_drop) # Drop from the original df\n",
    "        dropped_cols_str = \", \".join(cols_to_drop)\n",
    "        status = f\"Dropped {len(cols_to_drop)} columns ({dropped_cols_str}) with > {null_threshold_percent}% missing values.\"\n",
    "        print(f\"--- Status: {status} ---\")\n",
    "        return {\"dataframe\": df_cleaned.to_dict(), \"status\": status}\n",
    "\n",
    "def drop_all_null_rows(df: dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Drops rows from the DataFrame where all values are missing (NaN or empty string).\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing the modified DataFrame under the key 'dataframe'\n",
    "                        and a status message under the key 'status'.\n",
    "    \"\"\"\n",
    "    print(\"--- Calling drop_all_null_rows ---\")\n",
    "    if df is None:\n",
    "         return {\"dataframe\": None, \"status\": \"Error: No DataFrame provided.\"}\n",
    "\n",
    "    df = pd.DataFrame.from_dict(df)\n",
    "\n",
    "    initial_rows = len(df)\n",
    "    # Replace empty strings with NaN to use dropna effectively\n",
    "    df_copy = df.replace('', np.nan)\n",
    "    df_cleaned_temp = df_copy.dropna(how='all')\n",
    "\n",
    "    # Get the indices of rows that were kept\n",
    "    kept_indices = df_cleaned_temp.index\n",
    "\n",
    "    # Use the kept indices to filter the original DataFrame (preserving original values like '')\n",
    "    df_cleaned = df.loc[kept_indices]\n",
    "\n",
    "    rows_dropped = initial_rows - len(df_cleaned)\n",
    "\n",
    "    if rows_dropped > 0:\n",
    "        status = f\"Dropped {rows_dropped} rows where all values were missing.\"\n",
    "    else:\n",
    "        status = \"No all-missing rows found to drop.\"\n",
    "\n",
    "    print(f\"--- Status: {status} ---\")\n",
    "    return {\"dataframe\": df_cleaned.to_dict(), \"status\": status}\n",
    "\n",
    "\n",
    "def impute_missing_values(df: dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Imputes missing values (NaN or empty string) in the DataFrame.\n",
    "    Uses median for numerical columns and mode (most frequent) for categorical/object columns.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing the modified DataFrame under the key 'dataframe'\n",
    "                        and a status message under the key 'status'.\n",
    "    \"\"\"\n",
    "    print(\"--- Calling impute_missing_values ---\")\n",
    "    if df is None:\n",
    "         return {\"dataframe\": None, \"status\": \"Error: No DataFrame provided.\"}\n",
    "\n",
    "    df = pd.DataFrame.from_dict(df)\n",
    "\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned = df_cleaned.replace('', np.nan) # Treat empty strings as NaN for imputation\n",
    "\n",
    "    imputed_cols = []\n",
    "    numerical_cols = df_cleaned.select_dtypes(include=np.number).columns\n",
    "    categorical_cols = df_cleaned.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Impute numerical columns\n",
    "    if not df_cleaned[numerical_cols].isnull().sum().sum() == 0:\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "        df_cleaned[numerical_cols] = num_imputer.fit_transform(df_cleaned[numerical_cols])\n",
    "        imputed_cols.extend([col for col in numerical_cols if df[col].replace('', np.nan).isnull().any()]) # Check original df for missingness\n",
    "        print(f\"--- Imputed numerical columns (median): {[col for col in numerical_cols if df[col].replace('', np.nan).isnull().any()]} ---\")\n",
    "\n",
    "    # Impute categorical columns\n",
    "    if not df_cleaned[categorical_cols].isnull().sum().sum() == 0:\n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df_cleaned[categorical_cols] = cat_imputer.fit_transform(df_cleaned[categorical_cols])\n",
    "        imputed_cols.extend([col for col in categorical_cols if df[col].replace('', np.nan).isnull().any()]) # Check original df for missingness\n",
    "        print(f\"--- Imputed categorical columns (mode): {[col for col in categorical_cols if df[col].replace('', np.nan).isnull().any()]} ---\")\n",
    "\n",
    "    if imputed_cols:\n",
    "        status = f\"Imputed missing values in columns: {', '.join(sorted(list(set(imputed_cols))))} (Numerical: Median, Categorical: Mode).\"\n",
    "    else:\n",
    "        status = \"No missing values found to impute.\"\n",
    "\n",
    "    print(f\"--- Status: {status} ---\")\n",
    "    return {\"dataframe\": df_cleaned.to_dict(), \"status\": status}\n",
    "\n",
    "\n",
    "tools = [drop_high_null_columns, drop_all_null_rows, impute_missing_values]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1483808b-43ee-4fb0-a6af-88c442532f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools.base import BaseTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20564b8c-72f6-46a0-bc95-3e11edef229d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DropColumnTool(), DropRowTool(), ImputeTool()]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DropColumnTool(BaseTool):\n",
    "    name: str = \"drop_high_null_columns\"\n",
    "    description: str = \"\"\"\n",
    "    Drops columns from the DataFrame where the percentage of missing values\n",
    "    (NaN or empty string) exceeds the specified threshold.\n",
    "    \"\"\"\n",
    "    def _run(self, df):\n",
    "        return drop_high_null_columns(df)\n",
    "\n",
    "\n",
    "class DropRowTool(BaseTool):\n",
    "    name: str = \"drop_all_null_rows\"\n",
    "    description: str = \"\"\"\n",
    "    Drops rows from the DataFrame where all values are missing (NaN or empty string).\n",
    "    \"\"\"\n",
    "    def _run(self, df):\n",
    "        return drop_all_null_rows(df)\n",
    "\n",
    "\n",
    "class ImputeTool(BaseTool):\n",
    "    name: str = \"drop_all_null_rows\"\n",
    "    description: str = \"\"\"\n",
    "    Imputes missing values (NaN or empty string) in the DataFrame.\n",
    "    Uses median for numerical columns and mode (most frequent) for categorical/object columns.\n",
    "    \"\"\"\n",
    "    def _run(self, df):\n",
    "        return impute_missing_values(df)\n",
    "\n",
    "tools = [DropColumnTool(), DropRowTool(), ImputeTool()]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bb38602-697c-4f82-b1c4-6b9e14afa0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drop_high_null_columns', 'drop_all_null_rows', 'drop_all_null_rows']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.name for t in tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21bae226-7c99-465e-9735-6b5748f9b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Define Agent State ---\n",
    "class DataCleaningState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    dataframe: pd.DataFrame  # The actual data being cleaned\n",
    "    data_summary: str        # Text summary for the LLM\n",
    "    original_file_path: str  # Keep track of the source\n",
    "\n",
    "\n",
    "def call_data_cleaning_agent(state: DataCleaningState, config: RunnableConfig):\n",
    "    \"\"\"Invokes the LLM to decide the next cleaning step or generate a final response.\"\"\"\n",
    "    print(\"--- Calling LLM Agent ---\")\n",
    "    messages = state['messages']\n",
    "\n",
    "    current_summary = state['data_summary']  # Provide the latest data summary to the LLM\n",
    "    summary_message = HumanMessage(content=f\"Here is the current state of the data:\\n\\n{current_summary}\\n\\nBased *only* on this summary, what cleaning step should be taken next using the available tools? If the data looks sufficiently clean (e.g., no obvious high null columns, no all-null rows needing removal, and imputation has been attempted if needed), respond with a final assessment and do not call any tools.\")\n",
    "\n",
    "    model = llm\n",
    "    model_with_tools = model.bind_tools(tools)\n",
    "    llm_messages = messages + [summary_message]\n",
    "\n",
    "    print(\"--- Sending to LLM ---\")\n",
    "    print(\"Messages sent: \")\n",
    "    print([m.pretty_repr() for m in llm_messages])\n",
    "    response: AIMessage = model_with_tools.invoke(llm_messages, config=config)\n",
    "    print(f\"--- LLM Response: {response.content} ---\")\n",
    "    print(f\"--- LLM Tool Calls: {response.tool_calls} ---\")\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Node 2: Executes tools\n",
    "def execute_cleaning_tool(state: DataCleaningState):\n",
    "    \"\"\"Executes the cleaning tool called by the LLM.\"\"\"\n",
    "    print(\"--- Executing Tool ---\")\n",
    "    last_message: AIMessage = state['messages'][-1] # Get the latest AI message\n",
    "\n",
    "    if not last_message.tool_calls:\n",
    "        print(\"--- ERROR: No tool calls found in the last message, but Action node was reached. ---\")\n",
    "        # This case should ideally not be reached due to the conditional edge\n",
    "        # Add a message indicating potential confusion or end of planned tool use\n",
    "        no_tool_message = ToolMessage(content=\"No tool call was specified by the agent in the previous step.\", tool_call_id=\"error_no_tool\")\n",
    "        return {\"messages\": [no_tool_message]}\n",
    "\n",
    "    tool_messages: List[ToolMessage] = []\n",
    "    current_df = state['dataframe'] # Get the dataframe to operate on\n",
    "    new_df = current_df # Start with the current df\n",
    "\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        print(f\"--- Preparing Tool: {tool_name} ---\")\n",
    "        print(f\"--- Arguments: {tool_call['args']} ---\")\n",
    "\n",
    "        selected_tool = None\n",
    "        for t in tools:\n",
    "            if t.name == tool_name:\n",
    "                selected_tool = t\n",
    "                break\n",
    "\n",
    "        if selected_tool:\n",
    "            try:\n",
    "                tool_output_dict = selected_tool.invoke({**tool_call[\"args\"], \"df\": new_df})\n",
    "                new_df = tool_output_dict.get(\"dataframe\", new_df)\n",
    "                status_message = tool_output_dict.get(\"status\", \"Tool executed but provided no status.\")\n",
    "\n",
    "                print(f\"--- Tool Status: {status_message} ---\")\n",
    "                tool_messages.append(\n",
    "                    ToolMessage(content=status_message, tool_call_id=tool_call[\"id\"])\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"--- ERROR executing tool {tool_name}: {e} ---\")\n",
    "                error_message = f\"Error executing tool {tool_name}: {str(e)}\"\n",
    "                tool_messages.append(\n",
    "                    ToolMessage(content=error_message, tool_call_id=tool_call[\"id\"])\n",
    "                )\n",
    "        else:\n",
    "            print(f\"--- ERROR: Tool '{tool_name}' not found ---\")\n",
    "            tool_messages.append(\n",
    "                 ToolMessage(content=f\"Error: Tool '{tool_name}' not found.\", tool_call_id=tool_call[\"id\"])\n",
    "            )\n",
    "\n",
    "    new_summary = get_data_summary(new_df)\n",
    "    print(\"--- Data Summary Updated ---\")\n",
    "\n",
    "    return {\n",
    "        \"messages\": tool_messages,\n",
    "        \"dataframe\": new_df,\n",
    "        \"data_summary\": new_summary,\n",
    "    }\n",
    "\n",
    "def should_continue_cleaning(state: DataCleaningState) -> str:\n",
    "    \"\"\"Determines whether to continue cleaning (call a tool) or end.\"\"\"\n",
    "    print(\"--- Checking Condition: Should Continue Cleaning? ---\")\n",
    "    last_message = state['messages'][-1]\n",
    "\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        print(\"--- Decision: Continue (Execute Tool) ---\")\n",
    "        return \"execute_tool\"\n",
    "    else:\n",
    "        print(\"--- Decision: End Cleaning Cycle ---\")\n",
    "        return END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "117764fc-bb9f-45f9-85b5-dc839f4be15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workflow = StateGraph(DataCleaningState)\n",
    "workflow.add_node(\"agent\", call_data_cleaning_agent)\n",
    "workflow.add_node(\"execute_tool\", execute_cleaning_tool)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue_cleaning,\n",
    "    {\n",
    "        \"execute_tool\": \"execute_tool\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"execute_tool\", \"agent\")\n",
    "data_cleaning_app = workflow.compile()\n",
    "\n",
    "display(Image(data_cleaning_app.get_graph().draw_mermaid_png()))\n",
    "\n",
    "def run_data_cleaning_agent(file_path: str, output_file_path: str = \"cleaned_data.csv\"):\n",
    "    \"\"\"Loads data, runs the cleaning agent, and saves the result.\"\"\"\n",
    "    print(f\"\\n--- Starting Data Cleaning for: {file_path} ---\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    initial_summary = get_data_summary(df)\n",
    "    initial_state = DataCleaningState(\n",
    "        messages=[\n",
    "            SystemMessage(content=\"You are an expert data cleaning agent. Your goal is to iteratively clean the provided dataset using the available tools (`drop_high_null_columns`, `drop_all_null_rows`, `impute_missing_values`). Analyze the data summary provided at each step and decide the most appropriate cleaning action. Ask for clarification if needed, but prioritize using the tools to address issues like high null percentages, all-null rows, or missing values needing imputation. When you believe the data is sufficiently clean based on the summary, provide a final response summarizing the cleaning process and state that cleaning is complete, without calling any more tools.\"),\n",
    "            HumanMessage(content=f\"Please clean the data from the file: {os.path.basename(file_path)}. Start by analyzing the initial summary.\")\n",
    "        ],\n",
    "        dataframe=df,\n",
    "        data_summary=initial_summary,\n",
    "        original_file_path=file_path\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Running Cleaning Workflow ---\")\n",
    "    final_state = None\n",
    "    config = RunnableConfig(recursion_limit=10) \n",
    "    step_counter = 0\n",
    "    max_steps = 8\n",
    "\n",
    "    for output in data_cleaning_app.stream(initial_state, config=config, stream_mode=\"values\"):\n",
    "        step_counter += 1\n",
    "        print(f\"\\n--- Cleaning Step {step_counter} Completed ---\")\n",
    "        last_node = list(output.keys())[-1]\n",
    "        print(f\"--- Last Node Executed: {last_node} ---\")\n",
    "        # print(f\"--- Current Data Shape: {output['dataframe'].shape if output['dataframe'] is not None else 'N/A'} ---\")\n",
    "        # print(\"--- Current Messages ---\")\n",
    "        # for msg in output['messages']:\n",
    "        #      print(f\"  {msg.type.upper()}: {msg.content[:150]}...\") # Print truncated message content\n",
    "        # print(\"-\" * 30)\n",
    "        final_state = output # Keep track of the latest state\n",
    "        if step_counter >= max_steps:\n",
    "            print(f\"--- Reached maximum steps ({max_steps}), stopping execution. ---\")\n",
    "            break\n",
    "\n",
    "    if final_state is None:\n",
    "         print(\"--- ERROR: Workflow did not produce a final state. ---\")\n",
    "         return\n",
    "\n",
    "    # --- Process Final Result ---\n",
    "    print(\"\\n--- Cleaning Workflow Finished ---\")\n",
    "    final_df = final_state['dataframe']\n",
    "    final_summary = final_state['data_summary']\n",
    "    final_messages = final_state['messages']\n",
    "\n",
    "    print(\"\\n--- Final Data Summary ---\")\n",
    "    print(final_summary)\n",
    "\n",
    "    print(\"\\n--- Final Agent Message ---\")\n",
    "    if final_messages and isinstance(final_messages[-1], AIMessage):\n",
    "        print(final_messages[-1].content)\n",
    "    elif final_messages and isinstance(final_messages[-1], ToolMessage):\n",
    "         print(f\"(Ended after tool execution: {final_messages[-1].content})\")\n",
    "    else:\n",
    "        print(\"(No final AI message found)\")\n",
    "\n",
    "    if final_df is not None:\n",
    "        final_df.to_csv(output_file_path, index=False)\n",
    "        print(f\"\\n--- Cleaned data saved to: {output_file_path} ---\")\n",
    "    else:\n",
    "        print(\"--- WARNING: Final DataFrame is None, nothing to save. ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "280261f8-d22b-4a02-a40b-4b1999ea8e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thars\\AppData\\Local\\Temp\\ipykernel_18760\\2353215087.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dummy_df = pd.concat([dummy_df, all_null_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dummy CSV: dummy_data_to_clean.csv\n",
      "\n",
      "--- Starting Data Cleaning for: dummy_data_to_clean.csv ---\n",
      "\n",
      "--- Running Cleaning Workflow ---\n",
      "\n",
      "--- Cleaning Step 1 Completed ---\n",
      "--- Last Node Executed: original_file_path ---\n",
      "--- Calling LLM Agent ---\n",
      "--- Sending to LLM ---\n",
      "Messages sent: \n",
      "['================================ System Message ================================\\n\\nYou are an expert data cleaning agent. Your goal is to iteratively clean the provided dataset using the available tools (`drop_high_null_columns`, `drop_all_null_rows`, `impute_missing_values`). Analyze the data summary provided at each step and decide the most appropriate cleaning action. Ask for clarification if needed, but prioritize using the tools to address issues like high null percentages, all-null rows, or missing values needing imputation. When you believe the data is sufficiently clean based on the summary, provide a final response summarizing the cleaning process and state that cleaning is complete, without calling any more tools.', '================================ Human Message =================================\\n\\nPlease clean the data from the file: dummy_data_to_clean.csv. Start by analyzing the initial summary.', \"================================ Human Message =================================\\n\\nHere is the current state of the data:\\n\\nDataFrame Shape: (12, 8)\\n\\nMissing Values (NaN or ''):\\n- AllNullRowIndicator: 12 (100.00%)\\n- HighNullCol: 9 (75.00%)\\n- MixedNulls: 5 (41.67%)\\n- City: 4 (33.33%)\\n- Age: 4 (33.33%)\\n- Salary: 4 (33.33%)\\n- Name: 3 (25.00%)\\n- ID: 2 (16.67%)\\n\\nColumn Data Types:\\nID                     float64\\nName                    object\\nAge                    float64\\nCity                    object\\nSalary                 float64\\nHighNullCol            float64\\nAllNullRowIndicator    float64\\nMixedNulls             float64\\n\\nData Preview (first 3 rows):\\n    ID   Name   Age City   Salary  HighNullCol  AllNullRowIndicator  MixedNulls\\n0  1.0  Alice  25.0   NY  50000.0          NaN                  NaN        10.0\\n1  2.0    Bob  30.0   SF  60000.0          NaN                  NaN         NaN\\n2  3.0    NaN  35.0   LA  70000.0          NaN                  NaN        30.0\\n\\nBased *only* on this summary, what cleaning step should be taken next using the available tools? If the data looks sufficiently clean (e.g., no obvious high null columns, no all-null rows needing removal, and imputation has been attempted if needed), respond with a final assessment and do not call any tools.\"]\n",
      "--- LLM Response:  ---\n",
      "--- LLM Tool Calls: [{'name': 'drop_high_null_columns', 'args': {'df': {'ID': {}, 'Name': {}, 'Age': {}, 'City': {}, 'Salary': {}}}, 'id': 'call_7nzx', 'type': 'tool_call'}] ---\n",
      "--- Checking Condition: Should Continue Cleaning? ---\n",
      "--- Decision: Continue (Execute Tool) ---\n",
      "\n",
      "--- Cleaning Step 2 Completed ---\n",
      "--- Last Node Executed: original_file_path ---\n",
      "--- Executing Tool ---\n",
      "--- Preparing Tool: drop_high_null_columns ---\n",
      "--- Arguments: {'df': {'ID': {}, 'Name': {}, 'Age': {}, 'City': {}, 'Salary': {}}} ---\n",
      "--- Calling drop_high_null_columns (threshold: 50.0%) ---\n",
      "--- Status: Dropped 2 columns (HighNullCol, AllNullRowIndicator) with > 50.0% missing values. ---\n",
      "--- Tool Status: Dropped 2 columns (HighNullCol, AllNullRowIndicator) with > 50.0% missing values. ---\n",
      "--- Data Summary Updated ---\n",
      "\n",
      "--- Cleaning Step 3 Completed ---\n",
      "--- Last Node Executed: original_file_path ---\n",
      "--- Calling LLM Agent ---\n",
      "--- Sending to LLM ---\n",
      "Messages sent: \n",
      "['================================ System Message ================================\\n\\nYou are an expert data cleaning agent. Your goal is to iteratively clean the provided dataset using the available tools (`drop_high_null_columns`, `drop_all_null_rows`, `impute_missing_values`). Analyze the data summary provided at each step and decide the most appropriate cleaning action. Ask for clarification if needed, but prioritize using the tools to address issues like high null percentages, all-null rows, or missing values needing imputation. When you believe the data is sufficiently clean based on the summary, provide a final response summarizing the cleaning process and state that cleaning is complete, without calling any more tools.', '================================ Human Message =================================\\n\\nPlease clean the data from the file: dummy_data_to_clean.csv. Start by analyzing the initial summary.', \"================================== Ai Message ==================================\\nTool Calls:\\n  drop_high_null_columns (call_7nzx)\\n Call ID: call_7nzx\\n  Args:\\n    df: {'ID': {}, 'Name': {}, 'Age': {}, 'City': {}, 'Salary': {}}\", '================================= Tool Message =================================\\n\\nDropped 2 columns (HighNullCol, AllNullRowIndicator) with > 50.0% missing values.', \"================================ Human Message =================================\\n\\nHere is the current state of the data:\\n\\nDataFrame Shape: (12, 6)\\n\\nMissing Values (NaN or ''):\\n- MixedNulls: 5 (41.67%)\\n- Age: 4 (33.33%)\\n- City: 4 (33.33%)\\n- Salary: 4 (33.33%)\\n- Name: 3 (25.00%)\\n- ID: 2 (16.67%)\\n\\nColumn Data Types:\\nID            float64\\nName           object\\nAge           float64\\nCity           object\\nSalary        float64\\nMixedNulls    float64\\n\\nData Preview (first 3 rows):\\n    ID   Name   Age City   Salary  MixedNulls\\n0  1.0  Alice  25.0   NY  50000.0        10.0\\n1  2.0    Bob  30.0   SF  60000.0         NaN\\n2  3.0    NaN  35.0   LA  70000.0        30.0\\n\\nBased *only* on this summary, what cleaning step should be taken next using the available tools? If the data looks sufficiently clean (e.g., no obvious high null columns, no all-null rows needing removal, and imputation has been attempted if needed), respond with a final assessment and do not call any tools.\"]\n",
      "--- LLM Response:  ---\n",
      "--- LLM Tool Calls: [{'name': 'drop_all_null_rows', 'args': {'df': {'ID': {}, 'Name': {}, 'Age': {}, 'City': {}, 'Salary': {}}}, 'id': 'call_kmqk', 'type': 'tool_call'}] ---\n",
      "--- Checking Condition: Should Continue Cleaning? ---\n",
      "--- Decision: Continue (Execute Tool) ---\n",
      "\n",
      "--- Cleaning Step 4 Completed ---\n",
      "--- Last Node Executed: original_file_path ---\n",
      "--- Executing Tool ---\n",
      "--- Preparing Tool: drop_all_null_rows ---\n",
      "--- Arguments: {'df': {'ID': {}, 'Name': {}, 'Age': {}, 'City': {}, 'Salary': {}}} ---\n",
      "--- Calling drop_all_null_rows ---\n",
      "--- Status: Dropped 2 rows where all values were missing. ---\n",
      "--- Tool Status: Dropped 2 rows where all values were missing. ---\n",
      "--- Data Summary Updated ---\n",
      "\n",
      "--- Cleaning Step 5 Completed ---\n",
      "--- Last Node Executed: original_file_path ---\n",
      "--- Calling LLM Agent ---\n",
      "--- Sending to LLM ---\n",
      "Messages sent: \n",
      "['================================ System Message ================================\\n\\nYou are an expert data cleaning agent. Your goal is to iteratively clean the provided dataset using the available tools (`drop_high_null_columns`, `drop_all_null_rows`, `impute_missing_values`). Analyze the data summary provided at each step and decide the most appropriate cleaning action. Ask for clarification if needed, but prioritize using the tools to address issues like high null percentages, all-null rows, or missing values needing imputation. When you believe the data is sufficiently clean based on the summary, provide a final response summarizing the cleaning process and state that cleaning is complete, without calling any more tools.', '================================ Human Message =================================\\n\\nPlease clean the data from the file: dummy_data_to_clean.csv. Start by analyzing the initial summary.', \"================================== Ai Message ==================================\\nTool Calls:\\n  drop_high_null_columns (call_7nzx)\\n Call ID: call_7nzx\\n  Args:\\n    df: {'ID': {}, 'Name': {}, 'Age': {}, 'City': {}, 'Salary': {}}\", '================================= Tool Message =================================\\n\\nDropped 2 columns (HighNullCol, AllNullRowIndicator) with > 50.0% missing values.', \"================================== Ai Message ==================================\\nTool Calls:\\n  drop_all_null_rows (call_kmqk)\\n Call ID: call_kmqk\\n  Args:\\n    df: {'ID': {}, 'Name': {}, 'Age': {}, 'City': {}, 'Salary': {}}\", '================================= Tool Message =================================\\n\\nDropped 2 rows where all values were missing.', \"================================ Human Message =================================\\n\\nHere is the current state of the data:\\n\\nDataFrame Shape: (10, 6)\\n\\nMissing Values (NaN or ''):\\n- MixedNulls: 3 (30.00%)\\n- City: 2 (20.00%)\\n- Age: 2 (20.00%)\\n- Salary: 2 (20.00%)\\n- Name: 1 (10.00%)\\n\\nColumn Data Types:\\nID            float64\\nName           object\\nAge           float64\\nCity           object\\nSalary        float64\\nMixedNulls    float64\\n\\nData Preview (first 3 rows):\\n    ID   Name   Age City   Salary  MixedNulls\\n0  1.0  Alice  25.0   NY  50000.0        10.0\\n1  2.0    Bob  30.0   SF  60000.0         NaN\\n2  3.0    NaN  35.0   LA  70000.0        30.0\\n\\nBased *only* on this summary, what cleaning step should be taken next using the available tools? If the data looks sufficiently clean (e.g., no obvious high null columns, no all-null rows needing removal, and imputation has been attempted if needed), respond with a final assessment and do not call any tools.\"]\n",
      "--- LLM Response:  ---\n",
      "--- LLM Tool Calls: [{'name': 'drop_high_null_columns', 'args': {'df': {'ID': {}, 'Name': {}, 'Age': {}, 'City': {}, 'Salary': {}}, 'threshold': 0.3}, 'id': 'call_xx8r', 'type': 'tool_call'}] ---\n",
      "--- Checking Condition: Should Continue Cleaning? ---\n",
      "--- Decision: Continue (Execute Tool) ---\n",
      "\n",
      "--- Cleaning Step 6 Completed ---\n",
      "--- Last Node Executed: original_file_path ---\n",
      "--- Executing Tool ---\n",
      "--- Preparing Tool: drop_high_null_columns ---\n",
      "--- Arguments: {'df': {'ID': {}, 'Name': {}, 'Age': {}, 'City': {}, 'Salary': {}}, 'threshold': 0.3} ---\n",
      "--- ERROR executing tool drop_high_null_columns: DropColumnTool._run() got an unexpected keyword argument 'threshold' ---\n",
      "--- Data Summary Updated ---\n",
      "\n",
      "--- Cleaning Step 7 Completed ---\n",
      "--- Last Node Executed: original_file_path ---\n",
      "--- Calling LLM Agent ---\n",
      "--- Sending to LLM ---\n",
      "Messages sent: \n",
      "['================================ System Message ================================\\n\\nYou are an expert data cleaning agent. Your goal is to iteratively clean the provided dataset using the available tools (`drop_high_null_columns`, `drop_all_null_rows`, `impute_missing_values`). Analyze the data summary provided at each step and decide the most appropriate cleaning action. Ask for clarification if needed, but prioritize using the tools to address issues like high null percentages, all-null rows, or missing values needing imputation. When you believe the data is sufficiently clean based on the summary, provide a final response summarizing the cleaning process and state that cleaning is complete, without calling any more tools.', '================================ Human Message =================================\\n\\nPlease clean the data from the file: dummy_data_to_clean.csv. Start by analyzing the initial summary.', \"================================== Ai Message ==================================\\nTool Calls:\\n  drop_high_null_columns (call_7nzx)\\n Call ID: call_7nzx\\n  Args:\\n    df: {'ID': {}, 'Name': {}, 'Age': {}, 'City': {}, 'Salary': {}}\", '================================= Tool Message =================================\\n\\nDropped 2 columns (HighNullCol, AllNullRowIndicator) with > 50.0% missing values.', \"================================== Ai Message ==================================\\nTool Calls:\\n  drop_all_null_rows (call_kmqk)\\n Call ID: call_kmqk\\n  Args:\\n    df: {'ID': {}, 'Name': {}, 'Age': {}, 'City': {}, 'Salary': {}}\", '================================= Tool Message =================================\\n\\nDropped 2 rows where all values were missing.', \"================================== Ai Message ==================================\\nTool Calls:\\n  drop_high_null_columns (call_xx8r)\\n Call ID: call_xx8r\\n  Args:\\n    df: {'ID': {}, 'Name': {}, 'Age': {}, 'City': {}, 'Salary': {}}\\n    threshold: 0.3\", \"================================= Tool Message =================================\\n\\nError executing tool drop_high_null_columns: DropColumnTool._run() got an unexpected keyword argument 'threshold'\", \"================================ Human Message =================================\\n\\nHere is the current state of the data:\\n\\nDataFrame Shape: (10, 6)\\n\\nMissing Values (NaN or ''):\\n- MixedNulls: 3 (30.00%)\\n- City: 2 (20.00%)\\n- Age: 2 (20.00%)\\n- Salary: 2 (20.00%)\\n- Name: 1 (10.00%)\\n\\nColumn Data Types:\\nID            float64\\nName           object\\nAge           float64\\nCity           object\\nSalary        float64\\nMixedNulls    float64\\n\\nData Preview (first 3 rows):\\n    ID   Name   Age City   Salary  MixedNulls\\n0  1.0  Alice  25.0   NY  50000.0        10.0\\n1  2.0    Bob  30.0   SF  60000.0         NaN\\n2  3.0    NaN  35.0   LA  70000.0        30.0\\n\\nBased *only* on this summary, what cleaning step should be taken next using the available tools? If the data looks sufficiently clean (e.g., no obvious high null columns, no all-null rows needing removal, and imputation has been attempted if needed), respond with a final assessment and do not call any tools.\"]\n",
      "--- LLM Response:  ---\n",
      "--- LLM Tool Calls: [{'name': 'drop_all_null_rows', 'args': {'df': {'ID': {}, 'Name': {}, 'Age': {}, 'City': {}, 'Salary': {}}}, 'id': 'call_yjjc', 'type': 'tool_call'}] ---\n",
      "--- Checking Condition: Should Continue Cleaning? ---\n",
      "--- Decision: Continue (Execute Tool) ---\n",
      "\n",
      "--- Cleaning Step 8 Completed ---\n",
      "--- Last Node Executed: original_file_path ---\n",
      "--- Reached maximum steps (8), stopping execution. ---\n",
      "\n",
      "--- Cleaning Workflow Finished ---\n",
      "\n",
      "--- Final Data Summary ---\n",
      "DataFrame Shape: (10, 6)\n",
      "\n",
      "Missing Values (NaN or ''):\n",
      "- MixedNulls: 3 (30.00%)\n",
      "- City: 2 (20.00%)\n",
      "- Age: 2 (20.00%)\n",
      "- Salary: 2 (20.00%)\n",
      "- Name: 1 (10.00%)\n",
      "\n",
      "Column Data Types:\n",
      "ID            float64\n",
      "Name           object\n",
      "Age           float64\n",
      "City           object\n",
      "Salary        float64\n",
      "MixedNulls    float64\n",
      "\n",
      "Data Preview (first 3 rows):\n",
      "    ID   Name   Age City   Salary  MixedNulls\n",
      "0  1.0  Alice  25.0   NY  50000.0        10.0\n",
      "1  2.0    Bob  30.0   SF  60000.0         NaN\n",
      "2  3.0    NaN  35.0   LA  70000.0        30.0\n",
      "\n",
      "--- Final Agent Message ---\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated dummy CSV: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdummy_csv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Run the agent on the dummy CSV\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mrun_data_cleaning_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcleaned_dummy_data.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mrun_data_cleaning_agent\u001b[39m\u001b[34m(file_path, output_file_path)\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m(No final AI message found)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m final_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[43mfinal_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m(output_file_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     79\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Cleaned data saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "# --- 9. Example Usage ---\n",
    "\n",
    "# Create a dummy CSV for testing\n",
    "dummy_data = {\n",
    "    'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Name': ['Alice', 'Bob', '', 'David', 'Eve', 'Frank', 'Grace', 'Heidi', 'Ivan', 'Judy'],\n",
    "    'Age': [25, 30, 35, None, 28, 45, 22, 38, '', 50],\n",
    "    'City': ['NY', 'SF', 'LA', 'NY', None, 'SF', 'LA', 'SF', 'NY', ''],\n",
    "    'Salary': [50000, 60000, 70000, 55000, None, 90000, 45000, 80000, 65000, None],\n",
    "    'HighNullCol': [None] * 7 + [1, 2, 3], # >50% null\n",
    "    'AllNullRowIndicator': [None] * 10, # To test row drop\n",
    "    'MixedNulls': [10, None, 30, '', 50, None, 70, 80, 90, 100]\n",
    "}\n",
    "dummy_df = pd.DataFrame(dummy_data)\n",
    "# Add an all-null row\n",
    "all_null_row = pd.DataFrame([[None]*len(dummy_df.columns)], columns=dummy_df.columns)\n",
    "dummy_df = pd.concat([dummy_df, all_null_row], ignore_index=True)\n",
    "\n",
    "# Add another all-null row with empty strings\n",
    "all_empty_row = pd.DataFrame([['']*len(dummy_df.columns)], columns=dummy_df.columns)\n",
    "dummy_df = pd.concat([dummy_df, all_empty_row], ignore_index=True)\n",
    "\n",
    "dummy_csv_path = \"dummy_data_to_clean.csv\"\n",
    "dummy_df.to_csv(dummy_csv_path, index=False)\n",
    "print(f\"Created dummy CSV: {dummy_csv_path}\")\n",
    "\n",
    "# Run the agent on the dummy CSV\n",
    "run_data_cleaning_agent(dummy_csv_path, output_file_path=\"cleaned_dummy_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13f6d4ea-dc71-4cdc-b6a7-99b224d353b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function, convert_to_openai_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04699c78-ae66-4504-9ccf-ac51a0dfda61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'foo',\n",
       "  'description': '',\n",
       "  'parameters': {'properties': {'df': {'type': 'object'}},\n",
       "   'required': ['df'],\n",
       "   'type': 'object'}}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo(df: dict):\n",
    "    return df\n",
    "\n",
    "\n",
    "convert_to_openai_tool(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee47bb-19fb-42ae-be63-3b3b0687f42a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
