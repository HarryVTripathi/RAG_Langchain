{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "\n",
    " - implement document management\n",
    "\n",
    " - [implement re-ranking](https://www.pinecone.io/learn/series/rag/rerankers/)\n",
    "\n",
    " - Hyde: HyDE stands for Hypothetical Document Embeddings. It consists of two steps. First, it creates a hypothetical answer to a user query. Once the hypothetical answer/document is determined, the answer and the query are transformed into embeddings. Then, the system retrieves the documents closest to the embeddings in the vector space.\n",
    "\n",
    " - ensemble chunking\n",
    "\n",
    " - https://luv-bansal.medium.com/advance-rag-improve-rag-performance-208ffad5bb6a\n",
    "\n",
    " - Replacing pronouns with names in split chunks can enhance semantic significance during retrieval.\n",
    "\n",
    " - Metadata based vetor-store filters. The current user has access to which of the retrieved vectors.\n",
    "\n",
    " - [Chunk enrichment phase (post chunking)](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-enrichment-phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_y9ccuJe_TK_"
   },
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Chatmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fk9tNs8GkkJY",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "When a string is passed in as input, it is converted to a HumanMessage and then passed to the underlying model.\n",
    "\n",
    "LangChain does not host any Chat Models, rather we rely on third party integrations.\n",
    "\n",
    "We have some standardized parameters when constructing ChatModels:\n",
    "\n",
    "model: the name of the model\n",
    "temperature: the sampling temperature\n",
    "timeout: request timeout\n",
    "max_tokens: max tokens to generate\n",
    "stop: default stop sequences\n",
    "max_retries: max number of times to retry requests\n",
    "api_key: API key for the model provider\n",
    "base_url: endpoint to send requests to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSET3ecnk49k"
   },
   "source": [
    "Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n",
    "\n",
    "Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages. The reason this PromptValue exists is to make it easy to switch between strings and messages.\n",
    "\n",
    " - #### ChatPromptTemplates\n",
    "\n",
    "  These prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves. For example, a common way to construct and use a ChatPromptTemplate is as follows:\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})\n",
    "```\n",
    "\n",
    "#### MessagePlaceholder\n",
    "\n",
    "\n",
    "This prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw how we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would slot into a particular spot? This is how you use MessagesPlaceholder.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPWRhTChmOR-",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly.\n",
    "\n",
    "The concept of `ChatHistory` refers to a class in LangChain which can be used to wrap an arbitrary chain. This `ChatHistory` will keep track of inputs and outputs of the underlying chain, and append them as messages to a message database. Future interactions will then load those messages and pass them into the chain as part of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qo2ekmK9pliQ"
   },
   "source": [
    "## Trying it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lcpo7tLoMfws",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5txCJb4GF6_",
    "outputId": "7c6c3044-e7a6-4d42-b6f5-4809bb27c938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.0\n",
      "  Downloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-1.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.0 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1FBusOiMdlT"
   },
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install -qU langchain-groq\n",
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6tORC7wEobW"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQvkPucrEo9s"
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers==2.2.2 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting InstructorEmbedding\n",
      "  Obtaining dependency information for InstructorEmbedding from https://files.pythonhosted.org/packages/6c/fc/64375441f43cc9ddc81f76a1a8f516e6d63f5b6ecb67fffdcddc0445f0d3/InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata (20 kB)\n",
      "Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
      "Installing collected packages: InstructorEmbedding\n",
      "Successfully installed InstructorEmbedding-1.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install InstructorEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-YiygvUEv0j",
    "outputId": "2f0a66be-bd09-4a71-b97e-6e88a413cb88"
   },
   "outputs": [],
   "source": [
    "!pip install InstructorEmbedding\n",
    "!pip install langchain_chroma\n",
    "!pip install -qU faiss-cpu\n",
    "!pip install --upgrade --quiet pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubzj_lSgpoUL"
   },
   "source": [
    "### Create LLM groq provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yBDg0EXyof1Z"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMPg3T4XopQ6",
    "outputId": "9597c0c2-0340-4d6b-bd23-4dc4bd97d85e"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iDVt47JGoq0S"
   },
   "outputs": [],
   "source": [
    "LLAMA_8B = \"llama3-8b-8192\"\n",
    "LLAMA_70B = \"llama3-70b-8192\"\n",
    "GEMMA2_9B = \"gemma2-9b-it\"\n",
    "\n",
    "model = ChatGroq(model=LLAMA_8B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uxINd7Zg--f",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jf6RZmGcgpu2"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.runnables import RunnableSequence, RunnableMap, RunnableParallel\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUoxa5BtMW3r",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Basic chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqU9bBFm9ihc"
   },
   "source": [
    "```python\n",
    "system_msg = \"\"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\",\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\n",
    "      \"system\",\n",
    "      system_msg,\n",
    "    ),\n",
    "    (\"user\", \"Context: {context}\\n\\nQuestion: {question}\")\n",
    "  ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "with_message_history_args = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rts83uh2X7zh"
   },
   "source": [
    "### Refs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd6Z8uEcjgEh"
   },
   "source": [
    " - https://python.langchain.com/v0.2/docs/integrations/chat/google_generative_ai/\n",
    "\n",
    " - https://python.langchain.com/v0.2/docs/integrations/llms/google_vertex_ai_palm/\n",
    "\n",
    " - https://api.python.langchain.com/en/latest/chat_models/langchain_google_genai.chat_models.ChatGoogleGenerativeAI.html#langchain_google_genai.chat_models.ChatGoogleGenerativeAI\n",
    "\n",
    " -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3VqngQ16nTR"
   },
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkcVArf16p4D"
   },
   "source": [
    "$ \\large \\text{RAG is a technique for augmenting LLM knowledge with additional data.} $\n",
    "\n",
    "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVnZewjB617O"
   },
   "source": [
    "A typical RAG application has two main components:\n",
    "\n",
    "**Indexing**: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
    "\n",
    "**Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2r6MFOK9yKl"
   },
   "source": [
    "### Indexing\n",
    "\n",
    " 1. **Load**: First we need to load our data. This is done with [Document Loaders](https://python.langchain.com/v0.2/docs/concepts/#document-loaders).\n",
    "\n",
    " 2. **Split**: [Text splitters](https://python.langchain.com/v0.2/docs/concepts/#text-splitters) break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "\n",
    " At a high level, text splitters work as following:\n",
    "\n",
    "  - Split the text up into small, semantically meaningful chunks (often sentences).\n",
    "\n",
    "  - Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n",
    "\n",
    "  - Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n",
    "\n",
    " 3. **Store**: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a [VectorStore](https://python.langchain.com/v0.2/docs/concepts/#vector-stores) and [Embeddings model](https://python.langchain.com/v0.2/docs/concepts/#embedding-models).\n",
    "\n",
    " The Embeddings class is a class designed for interfacing with text embedding models. There are many different embedding model providers (OpenAI, Cohere, Hugging Face, etc) and local models, and this class is designed to provide a standard interface for all of them.\n",
    "\n",
    " > The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFLIPqEu-AFt"
   },
   "source": [
    "### Retrieval and generation\n",
    "\n",
    " 4. Retrieve: Given a user input, relevant splits are retrieved from storage using a [Retriever](https://python.langchain.com/v0.2/docs/concepts/#retrievers).\n",
    "\n",
    " 5. Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tSiSXkz96pUT"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3WZXZJWVqee"
   },
   "source": [
    "### Loading document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yqlTM80ISfLQ"
   },
   "outputs": [],
   "source": [
    "def read_pdf(path):\n",
    "  loader = PyMuPDFLoader(\n",
    "      path\n",
    "  )\n",
    "  doc = loader.load()\n",
    "  return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CR0k0LvikB9T",
    "outputId": "a5aed3b5-c075-4cf3-9455-73f4f17976c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thars\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:300: UserWarning: Warning: Empty content on page 0 of document ./A_Dance_With_Dragons.pdf\n",
      "  warnings.warn(\n",
      "C:\\Users\\thars\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:300: UserWarning: Warning: Empty content on page 4 of document ./A_Dance_With_Dragons.pdf\n",
      "  warnings.warn(\n",
      "C:\\Users\\thars\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:300: UserWarning: Warning: Empty content on page 5 of document ./A_Dance_With_Dragons.pdf\n",
      "  warnings.warn(\n",
      "C:\\Users\\thars\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:300: UserWarning: Warning: Empty content on page 6 of document ./A_Dance_With_Dragons.pdf\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_pdf(\"./A_Dance_With_Dragons.pdf\")\n",
    "len(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'—CASPOR HILL, HUMFREY STONE, MALO JAYN, DICK COLE, WILL COLE, LORIMAS MUDD, JON \\nLOTHSTON, LYMOND PEASE, SER BRENDEL BYRNE, DUNCAN STRONG, DENYS STRONG, CHAINS, YOUNG \\nJOHN MUDD, serjeants of the company, \\n—{SER AEGOR RIVERS, called BITTERSTEEL}, a bastard son of King Aegon IV Targaryen, founder \\nof the company}, \\n—{MAELYS I BLACKFYRE, called MAELYS THE MONSTROUS}, captain-general of the company, \\npretender to the Iron Throne of Westeros, member of the Band of Nine, slain during the War of the \\nNinepenny Kings, \\nTHE WINDBLOWN, two thousand horse and foot, sworn to Yunkai, \\n—THE TATTERED PRINCE, a former nobleman of the Free City of Pentos, captain and founder, \\n—CAGGO, called CORPSEKILLER, his right hand, \\n—DENZO D’HAN, the warrior bard, his left hand, \\n—HUGH HUNGERFORD, serjeant, former company paymaster, fined three fingers for stealing, \\n—SER ORSON STONE, SER LUCIFER LONG, WILL OF THE WOODS, DICK STRAW, GINJER JACK, \\nWesterosi sellswords, \\n—PRETTY MERIS, the company torturer, \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-5].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "K96hy2asBAQU"
   },
   "outputs": [],
   "source": [
    "data_filtered = data[7:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZ7VHItQBLos",
    "outputId": "6557c6d6-d49a-40f0-ae30-8bf9fb2af08e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfXFQHZUgvRK",
    "outputId": "02ea2f28-a2d9-4f5b-cae7-5dc9b71bae99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First page, starting content: \n",
      "PROLOGUE \n",
      "The night was rank with the smell of man. \n",
      "The warg stopped beneath a tree and sniffed, his grey-brown fur dappled by shadow. A sigh of \n",
      "piney wind brought the man-scent to him, over fainter smells that spoke of fox and hare, seal and stag, \n",
      "even wolf. Those were man-smells too, the warg knew; the stink of old skins, dead and sour, near \n",
      "drowned beneath the stronger scents of smoke and blood and rot. Only man stripped the skins from \n",
      "other beasts and wore their hides and hair. \n",
      "Wargs h\n"
     ]
    }
   ],
   "source": [
    "print(\"First page, starting content: \")\n",
    "print(data_filtered[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zpQwmZd1lCRv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last page, end content: \n",
      "—KASPORIO, called KASPORIO THE CUNNING, a bravo, second-in-command, \n",
      "—TYBERO ISTARION, called INKPOTS, company paymaster, \n",
      "—HAMMER, a drunken blacksmith and armorer, \n",
      "—his apprentice, called NAIL, \n",
      "—SNATCH, a serjeant, one-handed, \n",
      "—KEM, a young sellsword, from Flea Bottom, \n",
      "—BOKKOKO, an axeman of formidable repute, \n",
      "—UHLAN, a serjeant of the company, \n",
      "THE STORMCROWS, five hundred horse-riders, sworn to Queen Daenerys, \n",
      "—DAAERIO NAHARIS, captain and commander, \n",
      "—THE WIDOWER, his second-in-command, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Last page, end content: \")\n",
    "print(data_filtered[-1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmHtJOrwyT7o",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Splitting into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "osGuj9Q8gzse"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeW9EYYMmG_o"
   },
   "source": [
    "We set `add_start_index=True` so that the character index at which each split Document starts within the initial Document is preserved as metadata attribute “start_index”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8yKOfvorkWtb",
    "outputId": "42c8a0a7-a2b5-4d96-d46b-631fa7d3426b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3173"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_text(doc):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    "  )\n",
    "  all_splits = text_splitter.split_documents(doc)\n",
    "  return all_splits\n",
    "\n",
    "\n",
    "all_splits = split_text(data_filtered)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdT0TiQZ84cH",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Understanding chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlXJ4_HvmsFL"
   },
   "source": [
    "Note the `start_index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './A_Dance_With_Dragons.pdf',\n",
       " 'file_path': './A_Dance_With_Dragons.pdf',\n",
       " 'page': 8,\n",
       " 'total_pages': 894,\n",
       " 'format': 'PDF 1.5',\n",
       " 'title': 'A Dance With Dragons - A Song of Ice and Fire',\n",
       " 'author': 'George R R Martin',\n",
       " 'subject': 'nothuman',\n",
       " 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman',\n",
       " 'creator': 'Microsoft® Office Word 2007',\n",
       " 'producer': 'Microsoft® Office Word 2007',\n",
       " 'creationDate': 'D:20110711230404Z',\n",
       " 'modDate': \"D:20190506190144+02'00'\",\n",
       " 'trapped': '',\n",
       " 'start_index': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[3].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrsJxpiOmh5y",
    "outputId": "b2ec8417-f3e5-4e68-ed22-a4fd361665e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './A_Dance_With_Dragons.pdf',\n",
       " 'file_path': './A_Dance_With_Dragons.pdf',\n",
       " 'page': 891,\n",
       " 'total_pages': 894,\n",
       " 'format': 'PDF 1.5',\n",
       " 'title': 'A Dance With Dragons - A Song of Ice and Fire',\n",
       " 'author': 'George R R Martin',\n",
       " 'subject': 'nothuman',\n",
       " 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman',\n",
       " 'creator': 'Microsoft® Office Word 2007',\n",
       " 'producer': 'Microsoft® Office Word 2007',\n",
       " 'creationDate': 'D:20110711230404Z',\n",
       " 'modDate': \"D:20190506190144+02'00'\",\n",
       " 'trapped': '',\n",
       " 'start_index': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[-1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pC6MTlUtwrZ_",
    "outputId": "fdbd20fa-ed4c-4b7c-8f6a-2d2f33129d55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './A_Dance_With_Dragons.pdf', 'file_path': './A_Dance_With_Dragons.pdf', 'page': 891, 'total_pages': 894, 'format': 'PDF 1.5', 'title': 'A Dance With Dragons - A Song of Ice and Fire', 'author': 'George R R Martin', 'subject': 'nothuman', 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman', 'creator': 'Microsoft® Office Word 2007', 'producer': 'Microsoft® Office Word 2007', 'creationDate': 'D:20110711230404Z', 'modDate': \"D:20190506190144+02'00'\", 'trapped': '', 'start_index': 0}, page_content='—KASPORIO, called KASPORIO THE CUNNING, a bravo, second-in-command, \\n—TYBERO ISTARION, called INKPOTS, company paymaster, \\n—HAMMER, a drunken blacksmith and armorer, \\n—his apprentice, called NAIL, \\n—SNATCH, a serjeant, one-handed, \\n—KEM, a young sellsword, from Flea Bottom, \\n—BOKKOKO, an axeman of formidable repute, \\n—UHLAN, a serjeant of the company, \\nTHE STORMCROWS, five hundred horse-riders, sworn to Queen Daenerys, \\n—DAAERIO NAHARIS, captain and commander, \\n—THE WIDOWER, his second-in-command,')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='ced5253083a2d06e52962cfe5495233f', metadata={'source': './A_Dance_With_Dragons.pdf', 'file_path': './A_Dance_With_Dragons.pdf', 'page': 7, 'total_pages': 894, 'format': 'PDF 1.5', 'title': 'A Dance With Dragons - A Song of Ice and Fire', 'author': 'George R R Martin', 'subject': 'nothuman', 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman', 'creator': 'Microsoft® Office Word 2007', 'producer': 'Microsoft® Office Word 2007', 'creationDate': 'D:20110711230404Z', 'modDate': \"D:20190506190144+02'00'\", 'trapped': '', 'start_index': 0, 'role': 'registered_user'}, page_content='PROLOGUE \\nThe night was rank with the smell of man. \\nThe warg stopped beneath a tree and sniffed, his grey-brown fur dappled by shadow. A sigh of \\npiney wind brought the man-scent to him, over fainter smells that spoke of fox and hare, seal and stag, \\neven wolf. Those were man-smells too, the warg knew; the stink of old skins, dead and sour, near \\ndrowned beneath the stronger scents of smoke and blood and rot. Only man stripped the skins from \\nother beasts and wore their hides and hair. \\nWargs have no fear of man, as wolves do. Hate and hunger coiled in his belly, and he gave a low \\ngrowl, calling to his one-eyed brother, to his small sly sister. As he raced through the trees, his \\npackmates followed hard on his heels. They had caught the scent as well. As he ran, he saw through \\ntheir eyes too and glimpsed himself ahead. The breath of the pack puffed warm and white from long \\ngrey jaws. Ice had frozen between their paws, hard as stone, but the hunt was on now, the prey ahead.')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assign_id(all_splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_id(doc):\n",
    "    meta = doc.metadata\n",
    "    source = meta[\"source\"]\n",
    "    page = meta[\"page\"]\n",
    "    start_index = meta[\"start_index\"]\n",
    "\n",
    "    _id = bytes(f\"{source}:{page}:{start_index}\", 'utf-8')\n",
    "    h = md5(_id)\n",
    "    doc.id = h.hexdigest()\n",
    "    doc.metadata[\"role\"] = \"registered_user\"\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_with_id = list(map(assign_id , all_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='1c91c33d2a52bd7eb1ee5cdb139190f1', metadata={'source': './A_Dance_With_Dragons.pdf', 'file_path': './A_Dance_With_Dragons.pdf', 'page': 891, 'total_pages': 894, 'format': 'PDF 1.5', 'title': 'A Dance With Dragons - A Song of Ice and Fire', 'author': 'George R R Martin', 'subject': 'nothuman', 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman', 'creator': 'Microsoft® Office Word 2007', 'producer': 'Microsoft® Office Word 2007', 'creationDate': 'D:20110711230404Z', 'modDate': \"D:20190506190144+02'00'\", 'trapped': '', 'start_index': 0, 'role': 'registered_user'}, page_content='—KASPORIO, called KASPORIO THE CUNNING, a bravo, second-in-command, \\n—TYBERO ISTARION, called INKPOTS, company paymaster, \\n—HAMMER, a drunken blacksmith and armorer, \\n—his apprentice, called NAIL, \\n—SNATCH, a serjeant, one-handed, \\n—KEM, a young sellsword, from Flea Bottom, \\n—BOKKOKO, an axeman of formidable repute, \\n—UHLAN, a serjeant of the company, \\nTHE STORMCROWS, five hundred horse-riders, sworn to Queen Daenerys, \\n—DAAERIO NAHARIS, captain and commander, \\n—THE WIDOWER, his second-in-command,')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_with_id[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcmhX90e523Y",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "Z29Pb7ogwvnt",
    "outputId": "a56ff265-083f-4ac2-c6c3-bf4de3d388aa"
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "9Aqy71hC_frI",
    "outputId": "0b456e8e-a762-482c-a8fb-f735a94a08a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.0'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "id": "vvDMnjPckozi",
    "outputId": "abdb46c9-701b-4f07-b686-1b0d1da5514e"
   },
   "outputs": [],
   "source": [
    "model_name = \"hkunlp/instructor-large\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "hkunlp_embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dSbO4EEz9WGJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thars\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "mpnet = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "minilm = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=mpnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd8wcJ94zqJV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Store in vector database\n",
    "\n",
    "#### [Chroma ref](https://python.langchain.com/api_reference/chroma/vectorstores/langchain_chroma.vectorstores.Chroma.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.5'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain_community\n",
    "langchain_community.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "M5mJv0wVc102"
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRqjvII1ODL6",
    "outputId": "9dbec9ec-08d7-43ce-bcc4-cc63cc7d3a19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they came. Varamyr gave them his seed, took a hank of their hair to remember them by, and sent them \n",
      "back. From time to time, some village hero would come with spear in hand to slay the beastling and save \n",
      "a sister or a lover or a daughter. Those he killed, but he never harmed the women. Some he even \n",
      "blessed with children. Runts. Small, puny things, like Lump, and not one with the gift. \n",
      "Fear drove him to his feet, reeling. Holding his side to staunch the seep of blood from his \n",
      "wound, Varamyr lurched to the door and swept aside the ragged skin that covered it to face a wall of \n",
      "white. Snow. No wonder it had grown so dark and smoky inside. The falling snow had buried the hut. \n",
      "When Varamyr pushed at it, the snow crumbled and gave way, still soft and wet. Outside, the \n",
      "night was white as death; pale thin clouds danced attendance on a silver moon, while a thousand stars \n",
      "watched coldly. He could see the humped shapes of other huts buried beneath drifts of snow, and\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[21].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varamyr knew the truth of that. When he claimed the eagle that had been Orell’s, he could feel \n",
      "the other skinchanger raging at his presence. Orell had been slain by the turncloak crow Jon Snow, and \n",
      "his hate for his killer had been so strong that Varamyr found himself hating the beastling boy as well. He \n",
      "had known what Snow was the moment he saw that great white direwolf stalking silent at his side. One \n",
      "skinchanger can always sense another. Mance should have let me take the direwolf. There would be a \n",
      "second life worthy of a king. He could have done it, he did not doubt. The gift was strong in Snow, but \n",
      "the youth was untaught, still fighting his nature when he should have gloried in it. \n",
      "  \n",
      "Varamyr could see the weirwood’s red eyes staring down at him from the white trunk. The gods \n",
      "are weighing me. A shiver went through him. He had done bad things, terrible things. He had stolen,\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[32].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varamyr could see the weirwood’s red eyes staring down at him from the white trunk. The gods \n",
      "are weighing me. A shiver went through him. He had done bad things, terrible things. He had stolen, \n",
      "killed, raped. He had gorged on human flesh and lapped the blood of dying men as it gushed red and hot \n",
      "from their torn throats. He had stalked foes through the woods, fallen on them as they slept, clawed \n",
      "their entrails from their bellies and scattered them across the muddy earth. How sweet their meat had \n",
      "tasted. “That was the beast, not me,” he said in a hoarse whisper. “That was the gift you gave me.”\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[33].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gods made no reply. His breath hung pale and misty in the air. He could feel ice forming in \n",
      "his beard. Varamyr Sixskins closed his eyes. \n",
      "  \n",
      "He dreamt an old dream of a hovel by the sea, three dogs whimpering, a woman’s tears. \n",
      "  \n",
      "Bump. She weeps for Bump, but she never wept for me. \n",
      "  \n",
      "Lump had been born a month before his proper time, and he was sick so often that no one \n",
      "expected him to live. His mother waited until he was almost four to give him a proper name, and by \n",
      "then it was too late. The whole village had taken to calling him Lump, the name his sister Meha had \n",
      "given him when he was still in their mother’s belly. Meha had given Bump his name as well, but Lump’s \n",
      "little brother had been born in his proper time, big and red and robust, sucking greedily at Mother’s \n",
      "teats. She was going to name him after Father. Bump died, though. He died when he was two and I was \n",
      "six, three days before his nameday.\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[34].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "WK8fKS6ExgOs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 47min 52s\n",
      "Wall time: 32min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chroma_vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"./chroma_langchain_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 46min 37s\n",
      "Wall time: 32min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "faiss_vectorstore = FAISS.from_documents(documents=all_splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_R8yNC423TlR"
   },
   "source": [
    "Retriever: An object that returns Documents given a text query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "VUMji4m42nri"
   },
   "outputs": [],
   "source": [
    "q_s = [\n",
    "    \"Why according to Dornish law, Myrcella has stronger claim on the Iron Throne?\",\n",
    "    \"Who is the current King, Lord of the seven kingdoms, protector of the realm?\",\n",
    "    \"Who is his wife?\",\n",
    "    \"Who drank his way across the narrow sea?\",\n",
    "    \"What did Varamyr lose at the Wall?\",\n",
    "    \"From time to time, some village hero would come with spear in hand in order to slay the beastling, to save whom?\",\n",
    "    \"How many times had Varamyr died?\",\n",
    "    \"How did Oberyn die?\",\n",
    "    \"Who had Orell been slain by?\",\n",
    "    \"What was the name of crow who had slain Orell?\",\n",
    "    \"What do Jon Snow and Tormund Giantsbane talk about\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Using `similarity_search_with_relevance_scores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_docs = chroma_vectorstore.similarity_search_with_relevance_scores(q_s[0], k=4, score_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Preparations should be made for Princess Myrcella.” \n",
      "  \n",
      "“This is what comes of dealing with the Dornish,” Mace Tyrell said. “Surely a better match can \n",
      "be found for the girl?” \n",
      "  \n",
      "Such as your own son Willas, perhaps? Her disfigured by one Dornishman, him crippled by \n",
      "another? “No doubt,” Ser Kevan said, “but we have enemies enough without offending Dorne. If Doran \n",
      "Martell were to join his strength to Connington’s in support of this feigned dragon, things could go very \n",
      "ill for all of us.” \n",
      "  \n",
      "“Mayhaps we can persuade our Dornish friends to deal with Lord Connington,” Ser Harys Swyft \n",
      "said with an irritating titter. “That would save a deal of blood and trouble.”\n",
      "\n",
      "============================================================================================================================================\n",
      "\n",
      "their husbands and lovers. Better to live shamed than die proud, Ser Kevan told himself. “My niece will \n",
      "make no further mischief,” he promised Mace Tyrell. “You have my word on that, my lord.” \n",
      "  \n",
      "Tyrell gave a grudging nod. “As you say. My Margaery prefers to be tried by the Faith, so the \n",
      "whole realm can bear witness to her innocence.” \n",
      "  \n",
      "If your daughter is as innocent as you’d have us believe, why must you have your army present \n",
      "when she faces her accusers? Ser Kevan might have asked. “Soon, I hope,” he said instead, before \n",
      "turning to Grand Maester Pycelle. “Is there aught else?” \n",
      "  \n",
      "The Grand Maester consulted his papers. “We should address the Rosby inheritance. Six claims \n",
      "have been put forth—” \n",
      "  \n",
      "“We can settle Rosby at some later date. What else?” \n",
      "  \n",
      "“Preparations should be made for Princess Myrcella.” \n",
      "  \n",
      "“This is what comes of dealing with the Dornish,” Mace Tyrell said. “Surely a better match can \n",
      "be found for the girl?”\n",
      "\n",
      "============================================================================================================================================\n",
      "\n",
      "HOUSE MARTELL \n",
      " \n",
      "     \n",
      "Dorne was the last of the Seven Kingdoms to swear fealty to the Iron Throne. Blood, custom, \n",
      "geography, and history all helped to set the Dornishmen apart from the other kingdoms. At the \n",
      "outbreak of the War of the Five Kings Dorne took no part, but when Myrcella Baratheon was betrothed \n",
      "to Prince Trystane, Sunspear declared its support for King Joffrey. The Martell banner is a red sun \n",
      "pierced by a golden spear. Their words are Unbowed, Unbent, Unbroken. \n",
      "  \n",
      "DORAN NYMEROS MARTELL, Lord of Sunspear, Prince of Dorne, \n",
      " \n",
      "  \n",
      "—his wife, MELLARIO, of the Free City of Norvos, \n",
      " \n",
      "  \n",
      "—their children: \n",
      " \n",
      "  \n",
      "—PRINCESS ARIANNE, heir to Sunspear, \n",
      " \n",
      "  \n",
      "—PRINCE QUENTYN, a new-made knight, fostered at Yron-wood, \n",
      " \n",
      "  \n",
      "—PRINCE TRYSTANE, betrothed to Myrcella Baratheon, \n",
      " \n",
      "  \n",
      "—SER GASCOYNE OF THE GREENBLOOD, his sworn shield, \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      "  \n",
      "—his siblings: \n",
      " \n",
      "  \n",
      "—{PRINCESS ELIA}, raped and murdered during the Sack of King’s Landing,\n",
      "\n",
      "============================================================================================================================================\n",
      "\n",
      "at the Wall?” \n",
      "“Shivering, I would think. It is warmer down in Dorne. Perhaps he should have sailed that way.” \n",
      "Tyrion was beginning to suspect that a certain freckled washerwoman knew more of the \n",
      "Common Speech than she pretended. “My niece Myrcella is in Dorne, as it happens. And I have half a \n",
      "mind to make her a queen.” \n",
      "Illyrio smiled as his serving men spooned out bowls of black cherries in sweet cream for them \n",
      "both. “What has this poor child done to you that you would wish her dead?” \n",
      "“Even a kinslayer is not required to slay all his kin,” said Tyrion, wounded. “Queen her, I said. \n",
      "Not kill her.” \n",
      "The cheesemonger spooned up cherries. “In Volantis they use a coin with a crown on one face \n",
      "and a death’s-head on the other. Yet it is the same coin. To queen her is to kill her. Dorne might rise for \n",
      "Myrcella, but Dorne alone is not enough. If you are as clever as our friend insists, you know this.”\n"
     ]
    }
   ],
   "source": [
    "docs = [similar_doc[0].page_content for similar_doc in similar_docs]\n",
    "joiner = \"\"\"\n",
    "\n",
    "============================================================================================================================================\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(joiner.join(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Using retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZOj2C7q14wT",
    "outputId": "0d4e62eb-1617-4cee-b209-ae7036d07a4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = chroma_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5}, score_threshold=0.9)\n",
    "\n",
    "retrieved_docs = retriever.invoke(q_s[5])\n",
    "\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhyE8m6q2ff2",
    "outputId": "5cf0c638-c18d-4dae-85e2-03851f9fe7f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they came. Varamyr gave them his seed, took a hank of their hair to remember them by, and sent them \n",
      "back. From time to time, some village hero would come with spear in hand to slay the beastling and save \n",
      "a sister or a lover or a daughter. Those he killed, but he never harmed the women. Some he even \n",
      "blessed with children. Runts. Small, puny things, like Lump, and not one with the gift. \n",
      "Fear drove him to his feet, reeling. Holding his side to staunch the seep of blood from his \n",
      "wound, Varamyr lurched to the door and swept aside the ragged skin that covered it to face a wall of \n",
      "white. Snow. No wonder it had grown so dark and smoky inside. The falling snow had buried the hut. \n",
      "When Varamyr pushed at it, the snow crumbled and gave way, still soft and wet. Outside, the \n",
      "night was white as death; pale thin clouds danced attendance on a silver moon, while a thousand stars \n",
      "watched coldly. He could see the humped shapes of other huts buried beneath drifts of snow, and\n",
      "=============================================================================================\n",
      "The hero leaned into his spear, using his weight to twist the point in deeper. Drogon arched \n",
      "upward with a hiss of pain. His tail lashed sideways. She watched his head crane around at the end of \n",
      "that long serpentine neck, saw his black wings unfold. The dragonslayer lost his footing and went \n",
      "tumbling to the sand. He was trying to struggle back to his feet when the dragon’s teeth closed hard \n",
      "around his forearm. “No” was all the man had time to shout. Drogon wrenched his arm from his \n",
      "shoulder and tossed it aside as a dog might toss a rodent in a rat pit. \n",
      "  \n",
      "“Kill it,” Hizdahr zo Loraq shouted to the other spearmen. “Kill the beast!” \n",
      "  \n",
      "Ser Barristan held her tightly. “Look away, Your Grace.”\n",
      "=============================================================================================\n",
      "knights trying to avenge him. \n",
      "  \n",
      "Wun Weg Wun Dar Wun howled again and gave Ser Patrek’s other arm a twist and pull. It tore \n",
      "loose from his shoulder with a spray of bright red blood. Like a child pulling petals off a daisy, thought \n",
      "Jon. “Leathers, talk to him, calm him. The Old Tongue, he understands the Old Tongue. Keep back, the \n",
      "rest of you. Put away your steel, we’re scaring him.” Couldn’t they see the giant had been cut? Jon had \n",
      "to put an end to this or more men would die. They had no idea of Wun Wun’s strength. A horn, I need a \n",
      "horn. He saw the glint of steel, turned toward it. “No blades!” he screamed. “Wick, put that knife …” \n",
      "  \n",
      "… away, he meant to say. When Wick Whittlestick slashed at his throat, the word turned into a \n",
      "grunt. Jon twisted from the knife, just enough so it barely grazed his skin. He cut me. When he put his \n",
      "hand to the side of his neck, blood welled between his fingers. “Why?”\n",
      "=============================================================================================\n",
      "was on the floor, guttering, making every shadow leap and twist in a monstrous mockery of the dead \n",
      "man’s shaking. The prince never saw the locust’s spear coming toward him until Gerris slammed into \n",
      "him, knocking him aside. The spearpoint grazed the cheek of the lion’s head he wore. Even then the \n",
      "blow was so violent it almost tore the mask off. It would have gone right through my throat, the prince \n",
      "thought, dazed. \n",
      "  \n",
      "Gerris cursed as the locusts closed around him. Quentyn heard the sound of running feet. Then \n",
      "the sellswords came rushing from the shadows. One of the guards glanced at them just long enough for \n",
      "Gerris to get inside his spear. He drove the point of his sword under the brass mask and up through the \n",
      "wearer’s throat, even as the second locust sprouted a cross-bow bolt from his chest. \n",
      "  \n",
      "The last locust dropped his spear. “Yield. I yield.” \n",
      "  \n",
      "“No. You die.” Caggo took the man’s head off with one swipe of his arakh, the Valyrian steel\n",
      "=============================================================================================\n",
      "Next came Rattleshirt in clattering armor made of bones and boiled leather, his helm a giant’s skull. \n",
      "Under the bones lurked a ruined and wretched creature with cracked brown teeth and a yellow tinge to \n",
      "the whites of his eyes. A small, malicious, treacherous man, as stupid as he is cruel. Jon did not believe \n",
      "for a moment that he would keep faith. He wondered what Val was feeling as she watched him kneel, \n",
      "forgiven. \n",
      "  \n",
      "Lesser leaders followed. Two clan chiefs of the Hornfoot men, whose feet were black and hard. \n",
      "An old wisewoman revered by the peoples of the Milkwater. A scrawny dark-eyed boy of two-and-ten, \n",
      "the son of Alfyn Crowkiller. Halleck, brother to Harma Dogshead, with her pigs. Each took a knee before \n",
      "the king. \n",
      "  \n",
      "It is too cold for this mummer’s show, thought Jon. “The free folk despise kneelers,” he had \n",
      "warned Stannis. “Let them keep their pride, and they will love you better.” His Grace would not listen. \n",
      "He said, “It is swords I need from them, not kisses.”\n"
     ]
    }
   ],
   "source": [
    "docs = [retrieved_doc.page_content for retrieved_doc in retrieved_docs]\n",
    "\n",
    "print(\"\\n=============================================================================================\\n\".join(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import FlashrankRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = FlashrankRerank(top_n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Why did Tyrion Lannister kill his father?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_docs = chroma_vectorstore.similarity_search_with_relevance_scores(question, k=20, score_threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When he was still a lonely child in the depths of Casterly Rock, he oft rode dragons through the \n",
      "nights, pretending he was some lost Targaryen princeling, or a Valyrian dragonlord soaring high o’er \n",
      "fields and mountains. Once, when his uncles asked him what gift he wanted for his name-day, he \n",
      "begged them for a dragon. “It wouldn’t need to be a big one. It could be little, like I am.” His uncle \n",
      "Gerion thought that was the funniest thing he had ever heard, but his uncle Tygett said, “The last dragon \n",
      "died a century ago, lad.” That had seemed so monstrously unfair that the boy had cried himself to sleep \n",
      "that night. \n",
      "  \n",
      "Yet if the lord of cheese could be believed, the Mad King’s daughter had hatched three living \n",
      "dragons. Two more than even a Targaryen should require. Tyrion was almost sorry that he had killed his \n",
      "father. He would have enjoyed seeing Lord Tywin’s face when he learned that there was a Targaryen\n",
      "\n",
      "============================================================================================================================================\n",
      "\n",
      "known Tyrion Lannister, briefly. He took my hand and named me friend. It was hard to believe the little \n",
      "man had it in him to murder his own sire, but the fact of Lord Tywin’s demise seemed to be beyond \n",
      "doubt. “The lion in King’s Landing is a cub, and the Iron Throne has been known to cut grown men to \n",
      "ribbons.” \n",
      "  \n",
      "“A boy he may be, my lord, but … King Robert was well loved, and most men still accept that \n",
      "Tommen is his son. The more they see of Lord Stannis the less they love him, and fewer still are fond of \n",
      "Lady Melisandre with her fires and this grim red god of hers. They complain.” \n",
      "  \n",
      "“They complained about Lord Commander Mormont too. Men love to complain about their \n",
      "wives and lords, he told me once. Those without wives complain twice as much about their lords.” Jon \n",
      "Snow glanced toward the stockade. Two walls were down, a third falling fast. “I will leave you to finish \n",
      "here, Bowen. Make certain every corpse is burned. Thank you for your counsel. I promise you, I will\n",
      "\n",
      "============================================================================================================================================\n",
      "\n",
      "sellswords could boast of that? He hardly moves his lips at all, Tyrion reflected. \n",
      "  \n",
      "Finally Griff looked up from the parchment, and those pale eyes narrowed. “Tywin Lannister \n",
      "dead? At your hand?” \n",
      "  \n",
      "“At my finger. This one.” Tyrion held it up for Griff to admire. “Lord Tywin was sitting on a privy, \n",
      "so I put a crossbow bolt through his bowels to see if he really did shit gold. He didn’t. A pity, I could have \n",
      "used some gold. I also slew my mother, somewhat earlier. Oh, and my nephew Jof-frey, I poisoned him \n",
      "at his wedding feast and watched him choke to death. Did the cheesemonger leave that part out? I \n",
      "mean to add my brother and sister to the list before I’m done, if it please your queen.” \n",
      "  \n",
      "“Please her? Has Illyrio taken leave of his senses? Why does he imagine that Her Grace would \n",
      "welcome the service of a self-confessed kingslayer and betrayer?” \n",
      "  \n",
      "A fair question, thought Tyrion, but what he said was, “The king I slew was sitting on her throne,\n",
      "\n",
      "============================================================================================================================================\n",
      "\n",
      "Instead he had my father, Tyrion thought. “Some in the Free Cities think that we’re all savages \n",
      "on our side of the narrow sea,” the knight went on. “The ones who don’t think that we’re children, \n",
      "crying out for a father’s strong hand.” \n",
      "  \n",
      "“Or a mother’s?” Cersei will love that. Especially when he presents her with my head. “You seem \n",
      "to know this city well.” \n",
      "  \n",
      "“I spent the best part of a year here.” The knight sloshed the dregs at the bottom of his tankard. \n",
      "“When Stark drove me into exile, I fled to Lys with my second wife. Braavos would have suited me \n",
      "better, but Lynesse wanted someplace warm. Instead of serving the Braavosi I fought them on the \n",
      "Rhoyne, but for every silver I earned my wife spent ten. By the time I got back to Lys, she had taken a \n",
      "lover, who told me cheerfully that I would be enslaved for debt unless I gave her up and left the city. \n",
      "That was how I came to Volantis … one step ahead of slavery, owning nothing but my sword and the \n",
      "clothes upon my back.”\n",
      "\n",
      "============================================================================================================================================\n",
      "\n",
      "“Sick of losing to a dwarf, you mean?” \n",
      "  \n",
      "That pricked the lad’s pride, just as Tyrion had known it would. “Go fetch the board and pieces. \n",
      "This time I mean to smash you.” \n",
      "  \n",
      "They played on deck, sitting cross-legged behind the cabin. Young Griff arrayed his army for \n",
      "attack, with dragon, elephants, and heavy horse up front. A young man’s formation, as bold as it is \n",
      "foolish. He risks all for the quick kill. He let the prince have first move. Haldon stood behind them, \n",
      "watching the play. \n",
      "  \n",
      "When the prince reached for his dragon, Tyrion cleared his throat. “I would not do that if I were \n",
      "you. It is a mistake to bring your dragon out too soon.” He smiled innocently. “Your father knew the \n",
      "dangers of being over-bold.” \n",
      "  \n",
      "“Did you know my true father?” \n",
      "  \n",
      "“Well, I saw him twice or thrice, but I was only ten when Robert killed him, and mine own sire \n",
      "had me hidden underneath a rock. No, I cannot claim I knew Prince Rhaegar. Not as your false father\n"
     ]
    }
   ],
   "source": [
    "sim = [(doc[0].page_content, doc[1]) for doc in similar_docs]\n",
    "print(joiner.join([s[0] for s in sim[:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_docs = ranker.compress_documents([doc[0] for doc in similar_docs], question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'relevance_score': 0.9996016,\n",
       "  'page_content': 'known Tyrion Lannister, briefly. He took my hand and named me friend. It was hard to believe the little \\nman had it in him to murder his own sire, but the fact of Lord Tywin’s demise seemed to be beyond \\ndoubt. “The lion in King’s Landing is a cub, and the Iron Throne has been known to cut grown men to \\nribbons.” \\n  \\n“A boy he may be, my lord, but … King Robert was well loved, and most men still accept that \\nTommen is his son. The more they see of Lord Stannis the less they love him, and fewer still are fond of \\nLady Melisandre with her fires and this grim red god of hers. They complain.” \\n  \\n“They complained about Lord Commander Mormont too. Men love to complain about their \\nwives and lords, he told me once. Those without wives complain twice as much about their lords.” Jon \\nSnow glanced toward the stockade. Two walls were down, a third falling fast. “I will leave you to finish \\nhere, Bowen. Make certain every corpse is burned. Thank you for your counsel. I promise you, I will'},\n",
       " {'relevance_score': 0.999557,\n",
       "  'page_content': 'Tyrion, of House Lannister, rightful lord of Casterly Rock, you sniveling worm. “Yollo.” \\n  \\n“Bold Yollo. Bright Penny. You are the property of the noble and valorous Yezzan zo Qaggaz, \\nscholar and warrior, revered amongst the Wise Masters of Yunkai. Count yourselves fortunate, for \\nYezzan is a kindly and benevolent master. Think of him as you would your father.” \\n  \\nGladly, thought Tyrion, but this time he held his tongue. They would have to perform for their \\nnew master soon enough, he did not doubt, and he could not take another lash. \\n  \\n“Your father loves his special treasures best of all, and he will cherish you,” the overseer was \\nsaying. “And me, think of me as you would the nurse who cared for you when you were small. Nurse is \\nwhat all my children call me.” \\n  \\n“Lot ninety-nine,” the auctioneer called. “A warrior.” \\n  \\nThe girl had sold quickly and was being bundled off to her new owner, clutching her clothing to'},\n",
       " {'relevance_score': 0.999556,\n",
       "  'page_content': '—TYRION LANNISTER, called THE IMP, a dwarf, accused and condemned for regicide and \\nkinslaying, \\n \\n  \\n—his other kin: \\n \\n  \\n—his grandfather, {TYWIN LANNISTER}, Lord of Casterly Rock, Warden of the West, and Hand of \\nthe King, murdered in the privy by his son Tyrion, \\n \\n  \\n—his great-uncle, SER KEVAN LANNISTER, Lord Regent and Protector of the Realm, m. Dorna \\nSwyft, \\n \\n  \\n—their children: \\n \\n  \\n—SER LANCEL LANNISTER, a knight of the Holy Order of the Warrior’s Sons, \\n \\n  \\n—{WILLEM}, twin to Martyn, murdered at Riverrun, \\n \\n  \\n—MARTYN, twin to Willem, a squire, \\n \\n  \\n—JANEI, a girl of three, \\n \\n  \\n—his great-aunt, GENNA LANNISTER, m. Ser Emmon Frey, \\n \\n  \\n—their children: \\n \\n  \\n—{SER CLEOS FREY}, killed by outlaws,'},\n",
       " {'relevance_score': 0.99950975,\n",
       "  'page_content': 'sellswords could boast of that? He hardly moves his lips at all, Tyrion reflected. \\n  \\nFinally Griff looked up from the parchment, and those pale eyes narrowed. “Tywin Lannister \\ndead? At your hand?” \\n  \\n“At my finger. This one.” Tyrion held it up for Griff to admire. “Lord Tywin was sitting on a privy, \\nso I put a crossbow bolt through his bowels to see if he really did shit gold. He didn’t. A pity, I could have \\nused some gold. I also slew my mother, somewhat earlier. Oh, and my nephew Jof-frey, I poisoned him \\nat his wedding feast and watched him choke to death. Did the cheesemonger leave that part out? I \\nmean to add my brother and sister to the list before I’m done, if it please your queen.” \\n  \\n“Please her? Has Illyrio taken leave of his senses? Why does he imagine that Her Grace would \\nwelcome the service of a self-confessed kingslayer and betrayer?” \\n  \\nA fair question, thought Tyrion, but what he said was, “The king I slew was sitting on her throne,'},\n",
       " {'relevance_score': 0.9993515,\n",
       "  'page_content': '“Lannisters love tradition. Lend me your knife.” \\n  \\nInkpots raised an eyebrow, shrugged, slipped his dagger from its sheath, and handed it across \\nhiltfirst. It still hurts, Halfmaester, thank you very much, thought Tyrion, as he pricked the ball of his \\nthumb. He squeezed a fat drop of blood into the inkpot, traded the dagger for a fresh quill, and \\nscrawled, Tyrion of House Lannister, Lord of Casterly \\n  \\nRock, in a big bold hand, just below Jorah Mormont’s far more modest signature.'},\n",
       " {'relevance_score': 0.9992966,\n",
       "  'page_content': '“Sick of losing to a dwarf, you mean?” \\n  \\nThat pricked the lad’s pride, just as Tyrion had known it would. “Go fetch the board and pieces. \\nThis time I mean to smash you.” \\n  \\nThey played on deck, sitting cross-legged behind the cabin. Young Griff arrayed his army for \\nattack, with dragon, elephants, and heavy horse up front. A young man’s formation, as bold as it is \\nfoolish. He risks all for the quick kill. He let the prince have first move. Haldon stood behind them, \\nwatching the play. \\n  \\nWhen the prince reached for his dragon, Tyrion cleared his throat. “I would not do that if I were \\nyou. It is a mistake to bring your dragon out too soon.” He smiled innocently. “Your father knew the \\ndangers of being over-bold.” \\n  \\n“Did you know my true father?” \\n  \\n“Well, I saw him twice or thrice, but I was only ten when Robert killed him, and mine own sire \\nhad me hidden underneath a rock. No, I cannot claim I knew Prince Rhaegar. Not as your false father'}]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{\"relevance_score\": doc.metadata[\"relevance_score\"], \"page_content\": doc.page_content} for doc in reranked_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=ranker, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eT1rvRpe4roV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Hd2FmSCf4ukh"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "7fIntVOA4Vz2"
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NzuEyB0B2hT8"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\n",
    "      \"system\",\n",
    "      \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. Use three sentences maximum and keep the answer concise.\",\n",
    "    ),\n",
    "    (\"user\", \"Context: {context}\\n\\nQuestion: {question}\")\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CG3aVcR7l5I"
   },
   "source": [
    " - `retriever | format_docs` passes the question through the retriever, generating Document objects, and then to `format_docs` to generate strings;\n",
    "\n",
    " - `RunnablePassthrough()` passes through the input question unchanged. It is a Runnable to passthrough inputs unchanged or with additional keys.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chain can be composed as:\n",
    "\n",
    "```python\n",
    "context_question_map = RunnableMap({\n",
    "    \"context\": RunnableSequence([retriever, format_docs]),\n",
    "    \"question\": RunnablePassthrough()\n",
    "})\n",
    "\n",
    "rag_chain = RunnableSequence([\n",
    "    context_question_map,\n",
    "    prompt,\n",
    "    model,\n",
    "    StrOutputParser()\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "nq3zJaU22x-9"
   },
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": compression_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RkURV-R04fHz",
    "outputId": "9632e209-dcff-4670-990b-ec3e8e2b20fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tyrion killed his father, Lord Tywin Lannister, while he was sitting on the privy, shooting a crossbow bolt through his bowels."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(question):\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2IQoVsc4TCG",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using create stuff document chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69HbqK7z66MY",
    "outputId": "c260bdda-8d6b-41f7-80e9-95c0e43bc0f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 0.908081 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jon Snow and Tormund Giantsbane discuss a letter sent by Ramsay Bolton, demanding that Jon hand over his wildling princess, a wildling babe, and his Reek.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "VDi5RGY5-2D2"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "jMtd0bf99xm_"
   },
   "outputs": [],
   "source": [
    "qa_system_prompt = (\n",
    "  \"You are an assistant for question-answering tasks. \"\n",
    "  \"Use the following pieces of retrieved context to answer \"\n",
    "  \"the question. If you don't know the answer, say that you \"\n",
    "  \"don't know. Use five sentences maximum and try to keep the \"\n",
    "  \"answer concise.\"\n",
    "  \"\\n\\n\"\n",
    "  \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\"system\", qa_system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQQ_-zND_Yjc"
   },
   "source": [
    " - `create_stuff_documents_chain` specifies how retrieved context is fed into a prompt and LLM. In this case, we will \"stuff\" the contents into the prompt -- i.e., *we will include all retrieved context without any summarization or other processing.*\n",
    "\n",
    " It largely implements our above `rag_chain`, with input keys context and input-- it generates an answer using retrieved context and query.\n",
    "\n",
    " - `create_retrieval_chain` adds the retrieval step and propagates the retrieved context through the chain, providing it alongside the final answer. It has input key input, and includes input, context, and answer in its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "IgdcuwJE_GgI"
   },
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
    "qa_rag_chain = create_retrieval_chain(compression_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDNYRTKs_IbG",
    "outputId": "eebb1610-dce6-4e3d-fcce-fca585933136"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, Tyrion Lannister killed his father, Lord Tywin Lannister, by shooting him with a crossbow bolt while he was sitting on a privy. Tyrion claims he did it to see if Lord Tywin's feces would turn to gold, as was a long-standing rumor about the Lannister family, but ultimately found it to be a myth.\n"
     ]
    }
   ],
   "source": [
    "response = qa_rag_chain.invoke({\"input\": question})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aovElZXf_5AR",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Returning sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEIrQE___KyH",
    "outputId": "bafcc3ae-3023-4e2e-d978-54a2af594024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kingdoms will never be more ripe for conquest than they are right now. A boy king sits the Iron Throne. \n",
      "The north is in chaos, the riverlands a devastation, a rebel holds Storm’s End and Dragonstone. When \n",
      "winter comes, the realm will starve. And who remains to deal with all of this, who rules the little king \n",
      "who rules the Seven Kingdoms? Why, my own sweet sister. There is no one else. My brother, Jaime, \n",
      "thirsts for battle, not for power. He’s run from every chance he’s had to rule. My uncle Kevan would \n",
      "make a passably good regent if someone pressed the duty on him, but he will never reach for it. The \n",
      "gods shaped him to be a follower, not a leader.” Well, the gods and my lord father. “Mace Tyrell would \n",
      "grasp the sceptre gladly, but mine own kin are not like to step aside and give it to him. And everyone \n",
      "hates Stannis. Who does that leave? Why, only Cersei. \n",
      "  \n",
      "“Westeros is torn and bleeding, and I do not doubt that even now my sweet sister is binding up\n",
      "\n",
      "only man who can restore the realm and defend it against the peril that gathers in the north. Because he \n",
      "has a magic sword that glows with the light of the sun. The words caught in his throat. None of them\n",
      "\n",
      "—CHEZDHAR ZO RHAEZN, MAEZON ZO RHAEZN, GRAZDHAN ZO RHAEZN, noblemen and \n",
      "brothers, mocked as THE CLANKER LORDS, \n",
      " \n",
      "  \n",
      "—THE CHARIOTEER, THE BEASTMASTER, THE PERFUMED HERO, noblemen and slavers, \n",
      " \n",
      "  \n",
      "—in Astapor, the Red City: \n",
      " \n",
      "  \n",
      "—CLEON THE GREAT, called THE BUTCHER KING, \n",
      " \n",
      "  \n",
      "—CLEON II, his successor, king for eight days, \n",
      " \n",
      "  \n",
      "—KING CUTTHROAT, a barber, slit the throat of Cleon II to steal his crown, \n",
      " \n",
      "  \n",
      "—QUEEN WHORE, concubine to King Cleon II, claimed the throne after his murder. \n",
      " \n",
      "  \n",
      "THE QUEEN ACROSS THE WATER \n",
      " \n",
      "     \n",
      "DAENERYS TARGARYEN, the First of Her Name, Queen of Meereen, Queen of the Andals and the \n",
      "Rhoynar and the First Men, Lord of the Seven Kingdoms, Protector of the Realm, Khaleesi of the Great \n",
      "Grass Sea, called DAENERYS STORMBORN, the UNBURNT, MOTHER OF DRAGONS, \n",
      "  \n",
      "—her dragons, DROGON, VISERION, RHAEGAL, \n",
      " \n",
      "  \n",
      "—her brother, {RHAEGAR}, Prince of Dragonstone, slain by Robert Baratheon on the Trident,\n",
      "\n",
      "—her sworn shield, {SER ARYS OAKHEART}, slain by Areo Hotah, \n",
      " \n",
      "  \n",
      "—her bedmaid and companion, ROSAMUND LANNISTER, a distant cousin, \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      "  \n",
      "—his bannermen, the Lords of Dorne: \n",
      " \n",
      "  \n",
      "—ANDERS YRONWOOD, Lord of Yronwood, Warden of the Stone Way, the Bloodroyal, \n",
      " \n",
      "  \n",
      "—YNYS, his eldest daughter, m. Ryon Allyrion, \n",
      " \n",
      "  \n",
      "—SER CLETUS, his son and heir, \n",
      " \n",
      "  \n",
      "—GWYNETH, his youngest daughter, a girl of twelve, \n",
      " \n",
      "  \n",
      "—HARMEN ULLER, Lord of Hellholt, \n",
      " \n",
      "  \n",
      "—DELONNE ALLYRION, Lady of Godsgrace, \n",
      " \n",
      "  \n",
      "—RYON ALLYRION, her son and heir, \n",
      " \n",
      "  \n",
      "—DAGOS MANWOODY, Lord of Kingsgrave, \n",
      " \n",
      "  \n",
      "—LARRA BLACKMONT, Lady of Blackmont,\n",
      "\n",
      "—LADY MARGOT, a cousin, m. Lord Titus Peake, \n",
      "—bannermen and sworn swords, Lords of the West: \n",
      "—DAMON MARBRAND, Lord of Ashemark, \n",
      "—ROLAND CRAKEHALL, Lord of Crakehall, \n",
      "—SEBASTON FARMAN, Lord of Fair Isle, \n",
      "—TYTOS BRAX, Lord of Hornvale, \n",
      "—QUENTEN BANEFORT, Lord of Banefort, \n",
      "—SER HARYS SWYFT, goodfather to Ser Kevan Lannister, \n",
      "—REGENARD ESTREN, Lord of Wyndhall, \n",
      "—GAWEN WESTERLING, Lord of the Crag, \n",
      "—LORD SELMOND STACKSPEAR, \n",
      "—TERRENCE KENNING, Lord of Kayce,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document in response[\"context\"]:\n",
    "  print(document.page_content)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chatting with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Vanilla way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"history\"),\n",
    "        (\"human\", \"Context: {context}\\n\\nQuestion: {question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    RunnableParallel({\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"history\": RunnablePassthrough(),\n",
    "    })\n",
    "    | qa_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "conversational_rag_chain_simple = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "config_simple = {\"configurable\": {\"session_id\": \"abc1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response_1 \u001b[38;5;241m=\u001b[39m \u001b[43mconversational_rag_chain_simple\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho was Varamyr?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_simple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5356\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5356\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5356\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4713\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   4699\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[0;32m   4700\u001b[0m \n\u001b[0;32m   4701\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4710\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[0;32m   4711\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 4713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4714\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4715\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4716\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4717\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4718\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4719\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4720\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4721\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4722\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4723\u001b[0m     )\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1927\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1923\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1924\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1925\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1926\u001b[0m         Output,\n\u001b[1;32m-> 1927\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1930\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1933\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1935\u001b[0m     )\n\u001b[0;32m   1936\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1937\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4578\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   4574\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4575\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit reached when invoking \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4576\u001b[0m         )\n\u001b[0;32m   4577\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRecursionError\u001b[39;00m(msg)\n\u001b[1;32m-> 4578\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4579\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4582\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4583\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrecursion_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecursion_limit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4584\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4585\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(Output, output)\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5356\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3024\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3727\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3722\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3723\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3724\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[0;32m   3725\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3726\u001b[0m         ]\n\u001b[1;32m-> 3727\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   3728\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Python\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mC:\\Python\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3711\u001b[0m, in \u001b[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001b[1;34m(step, input, config, key)\u001b[0m\n\u001b[0;32m   3709\u001b[0m context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   3710\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m-> 3711\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3713\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3024\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\retrievers.py:254\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    253\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[0;32m    257\u001b[0m         result,\n\u001b[0;32m    258\u001b[0m     )\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\retrievers.py:247\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 247\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1080\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[0;32m   1078\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1080\u001b[0m         docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1081\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1082\u001b[0m         docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[0;32m   1084\u001b[0m                 query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs\n\u001b[0;32m   1085\u001b[0m             )\n\u001b[0;32m   1086\u001b[0m         )\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:582\u001b[0m, in \u001b[0;36mChroma.similarity_search\u001b[1;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    566\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    570\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run similarity search with Chroma.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \n\u001b[0;32m    573\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;124;03m        List of documents most similar to the query text.\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 582\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:679\u001b[0m, in \u001b[0;36mChroma.similarity_search_with_score\u001b[1;34m(self, query, k, filter, where_document, **kwargs)\u001b[0m\n\u001b[0;32m    671\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__query_collection(\n\u001b[0;32m    672\u001b[0m         query_texts\u001b[38;5;241m=\u001b[39m[query],\n\u001b[0;32m    673\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    677\u001b[0m     )\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 679\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    680\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__query_collection(\n\u001b[0;32m    681\u001b[0m         query_embeddings\u001b[38;5;241m=\u001b[39m[query_embedding],\n\u001b[0;32m    682\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _results_to_docs_and_scores(results)\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:108\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_query\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute query embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m        Embeddings for the text.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:79\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute doc embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_process:\n\u001b[0;32m     81\u001b[0m     pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mstart_multi_process_pool()\n",
      "File \u001b[1;32m~\\Documents\\DS\\RAG\\RAG_Langchain\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:79\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute doc embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m), texts))\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_process:\n\u001b[0;32m     81\u001b[0m     pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mstart_multi_process_pool()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "response_1 = conversational_rag_chain_simple.invoke(\n",
    "    {\"question\": \"Who was Varamyr?\"},\n",
    "    config=config_simple,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `create_retrieval_chain` and `create_history_aware_retriever`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    model, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
    "rag_qa_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_qa_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_1 = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Who was Varamyr?\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Varamyr was a skinchanger, also known as Varamyr Six-skins, who could shape-shift into various animals, including bears and eagles. He was originally named Lump and was taught the ways of the skinchanger by Haggon.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_1[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2 = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"How many times did he die?\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Varamyr died nine times before the events described in the passage.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_2[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_3 = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Who was Orell?\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Orell was a skinchanger who had an eagle as his bonded animal. He was slain by Jon Snow, also known as the turncloak crow.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_3[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_4 = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"How did he die?\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Orell was slain by Jon Snow, the turncloak crow, while he was riding his eagle.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_4[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chatting with RAG v0.3\n",
    "\n",
    " - [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver)\n",
    "\n",
    "A graph whose nodes communicate by reading and writing to a shared state. \n",
    "\n",
    " - [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)\n",
    "\n",
    "An in-memory checkpoint saver.\r\n",
    "\r\n",
    "This checkpoint saver stores checkpoints in memory using a defaultdict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = faiss_vectorstore.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = model.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to Dornish law, Myrcella has a stronger claim on the Iron Throne because she is married to Prince Trystane Martell, making her a princess of Dorne.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": q_s[0]})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import MessagesState, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = faiss_vectorstore.similarity_search(query, k=6)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using graph builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = model.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "# A node for the retriever tool that executes the retrieval step;\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "# A node that generates the final response using the retrieved context.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]  # retrieval tools\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise. Pay attention to names of characters.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Who is the current King, Lord of the seven kingdoms, protector of the realm?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_j0vw)\n",
      " Call ID: call_j0vw\n",
      "  Args:\n",
      "    query: Who is the current King, Lord of the seven kingdoms, protector of the realm?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': './A_Dance_With_Dragons.pdf', 'file_path': './A_Dance_With_Dragons.pdf', 'page': 115, 'total_pages': 894, 'format': 'PDF 1.5', 'title': 'A Dance With Dragons - A Song of Ice and Fire', 'author': 'George R R Martin', 'subject': 'nothuman', 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman', 'creator': 'Microsoft® Office Word 2007', 'producer': 'Microsoft® Office Word 2007', 'creationDate': 'D:20110711230404Z', 'modDate': \"D:20190506190144+02'00'\", 'trapped': '', 'start_index': 3383}\n",
      "Content: only man who can restore the realm and defend it against the peril that gathers in the north. Because he \n",
      "has a magic sword that glows with the light of the sun. The words caught in his throat. None of them\n",
      "\n",
      "Source: {'source': './A_Dance_With_Dragons.pdf', 'file_path': './A_Dance_With_Dragons.pdf', 'page': 241, 'total_pages': 894, 'format': 'PDF 1.5', 'title': 'A Dance With Dragons - A Song of Ice and Fire', 'author': 'George R R Martin', 'subject': 'nothuman', 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman', 'creator': 'Microsoft® Office Word 2007', 'producer': 'Microsoft® Office Word 2007', 'creationDate': 'D:20110711230404Z', 'modDate': \"D:20190506190144+02'00'\", 'trapped': '', 'start_index': 823}\n",
      "Content: Kingdoms will never be more ripe for conquest than they are right now. A boy king sits the Iron Throne. \n",
      "The north is in chaos, the riverlands a devastation, a rebel holds Storm’s End and Dragonstone. When \n",
      "winter comes, the realm will starve. And who remains to deal with all of this, who rules the little king \n",
      "who rules the Seven Kingdoms? Why, my own sweet sister. There is no one else. My brother, Jaime, \n",
      "thirsts for battle, not for power. He’s run from every chance he’s had to rule. My uncle Kevan would \n",
      "make a passably good regent if someone pressed the duty on him, but he will never reach for it. The \n",
      "gods shaped him to be a follower, not a leader.” Well, the gods and my lord father. “Mace Tyrell would \n",
      "grasp the sceptre gladly, but mine own kin are not like to step aside and give it to him. And everyone \n",
      "hates Stannis. Who does that leave? Why, only Cersei. \n",
      "  \n",
      "“Westeros is torn and bleeding, and I do not doubt that even now my sweet sister is binding up\n",
      "\n",
      "Source: {'source': './A_Dance_With_Dragons.pdf', 'file_path': './A_Dance_With_Dragons.pdf', 'page': 844, 'total_pages': 894, 'format': 'PDF 1.5', 'title': 'A Dance With Dragons - A Song of Ice and Fire', 'author': 'George R R Martin', 'subject': 'nothuman', 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman', 'creator': 'Microsoft® Office Word 2007', 'producer': 'Microsoft® Office Word 2007', 'creationDate': 'D:20110711230404Z', 'modDate': \"D:20190506190144+02'00'\", 'trapped': '', 'start_index': 0}\n",
      "Content: —LADY MARGOT, a cousin, m. Lord Titus Peake, \n",
      "—bannermen and sworn swords, Lords of the West: \n",
      "—DAMON MARBRAND, Lord of Ashemark, \n",
      "—ROLAND CRAKEHALL, Lord of Crakehall, \n",
      "—SEBASTON FARMAN, Lord of Fair Isle, \n",
      "—TYTOS BRAX, Lord of Hornvale, \n",
      "—QUENTEN BANEFORT, Lord of Banefort, \n",
      "—SER HARYS SWYFT, goodfather to Ser Kevan Lannister, \n",
      "—REGENARD ESTREN, Lord of Wyndhall, \n",
      "—GAWEN WESTERLING, Lord of the Crag, \n",
      "—LORD SELMOND STACKSPEAR, \n",
      "—TERRENCE KENNING, Lord of Kayce,\n",
      "\n",
      "Source: {'source': './A_Dance_With_Dragons.pdf', 'file_path': './A_Dance_With_Dragons.pdf', 'page': 823, 'total_pages': 894, 'format': 'PDF 1.5', 'title': 'A Dance With Dragons - A Song of Ice and Fire', 'author': 'George R R Martin', 'subject': 'nothuman', 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman', 'creator': 'Microsoft® Office Word 2007', 'producer': 'Microsoft® Office Word 2007', 'creationDate': 'D:20110711230404Z', 'modDate': \"D:20190506190144+02'00'\", 'trapped': '', 'start_index': 0}\n",
      "Content: —{CRAGORN}, who blew the hellhorn and died, \n",
      "—his lords bannermen: \n",
      "—ERIK IRONMAKER, called ERIK ANVIL-BREAKER and ERIK THE JUST, Lord Steward of the Iron \n",
      "Islands, castellan of Pyke, an old man once renowned, m. Asha Greyjoy, \n",
      "—lords of Pyke: \n",
      "—GERMUND BOTLEY, Lord of Lordsport, \n",
      "—WALDON WYNCH, Lord of Iron Holt, \n",
      "—lords of Old Wyk: \n",
      "—DUNSTAN DRUMM, The Drumm, Lord of Old Wyk, \n",
      "—NORNE GOODBROTHER, of Shatterstone, \n",
      "—THE STONEHOUSE, \n",
      "—lords of Great Wyk: \n",
      "—GOROLD GOODBROTHER, Lord of the Hammerhorn, \n",
      "—TRISTON FARWYND, Lord of Sealskin Point,\n",
      "\n",
      "Source: {'source': './A_Dance_With_Dragons.pdf', 'file_path': './A_Dance_With_Dragons.pdf', 'page': 849, 'total_pages': 894, 'format': 'PDF 1.5', 'title': 'A Dance With Dragons - A Song of Ice and Fire', 'author': 'George R R Martin', 'subject': 'nothuman', 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman', 'creator': 'Microsoft® Office Word 2007', 'producer': 'Microsoft® Office Word 2007', 'creationDate': 'D:20110711230404Z', 'modDate': \"D:20190506190144+02'00'\", 'trapped': '', 'start_index': 3}\n",
      "Content: —her sworn shield, {SER ARYS OAKHEART}, slain by Areo Hotah, \n",
      " \n",
      "  \n",
      "—her bedmaid and companion, ROSAMUND LANNISTER, a distant cousin, \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      "  \n",
      "—his bannermen, the Lords of Dorne: \n",
      " \n",
      "  \n",
      "—ANDERS YRONWOOD, Lord of Yronwood, Warden of the Stone Way, the Bloodroyal, \n",
      " \n",
      "  \n",
      "—YNYS, his eldest daughter, m. Ryon Allyrion, \n",
      " \n",
      "  \n",
      "—SER CLETUS, his son and heir, \n",
      " \n",
      "  \n",
      "—GWYNETH, his youngest daughter, a girl of twelve, \n",
      " \n",
      "  \n",
      "—HARMEN ULLER, Lord of Hellholt, \n",
      " \n",
      "  \n",
      "—DELONNE ALLYRION, Lady of Godsgrace, \n",
      " \n",
      "  \n",
      "—RYON ALLYRION, her son and heir, \n",
      " \n",
      "  \n",
      "—DAGOS MANWOODY, Lord of Kingsgrave, \n",
      " \n",
      "  \n",
      "—LARRA BLACKMONT, Lady of Blackmont,\n",
      "\n",
      "Source: {'source': './A_Dance_With_Dragons.pdf', 'file_path': './A_Dance_With_Dragons.pdf', 'page': 879, 'total_pages': 894, 'format': 'PDF 1.5', 'title': 'A Dance With Dragons - A Song of Ice and Fire', 'author': 'George R R Martin', 'subject': 'nothuman', 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman', 'creator': 'Microsoft® Office Word 2007', 'producer': 'Microsoft® Office Word 2007', 'creationDate': 'D:20110711230404Z', 'modDate': \"D:20190506190144+02'00'\", 'trapped': '', 'start_index': 5}\n",
      "Content: —CHEZDHAR ZO RHAEZN, MAEZON ZO RHAEZN, GRAZDHAN ZO RHAEZN, noblemen and \n",
      "brothers, mocked as THE CLANKER LORDS, \n",
      " \n",
      "  \n",
      "—THE CHARIOTEER, THE BEASTMASTER, THE PERFUMED HERO, noblemen and slavers, \n",
      " \n",
      "  \n",
      "—in Astapor, the Red City: \n",
      " \n",
      "  \n",
      "—CLEON THE GREAT, called THE BUTCHER KING, \n",
      " \n",
      "  \n",
      "—CLEON II, his successor, king for eight days, \n",
      " \n",
      "  \n",
      "—KING CUTTHROAT, a barber, slit the throat of Cleon II to steal his crown, \n",
      " \n",
      "  \n",
      "—QUEEN WHORE, concubine to King Cleon II, claimed the throne after his murder. \n",
      " \n",
      "  \n",
      "THE QUEEN ACROSS THE WATER \n",
      " \n",
      "     \n",
      "DAENERYS TARGARYEN, the First of Her Name, Queen of Meereen, Queen of the Andals and the \n",
      "Rhoynar and the First Men, Lord of the Seven Kingdoms, Protector of the Realm, Khaleesi of the Great \n",
      "Grass Sea, called DAENERYS STORMBORN, the UNBURNT, MOTHER OF DRAGONS, \n",
      "  \n",
      "—her dragons, DROGON, VISERION, RHAEGAL, \n",
      " \n",
      "  \n",
      "—her brother, {RHAEGAR}, Prince of Dragonstone, slain by Robert Baratheon on the Trident,\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "According to the context, the current King, Lord of the Seven Kingdoms, and Protector of the Realm is King Tommen Baratheon, also known as the \"boy king\" who sits the Iron Throne.\n"
     ]
    }
   ],
   "source": [
    "input_message = q_s[1]\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Who is his wife?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_d8e5)\n",
      " Call ID: call_d8e5\n",
      "  Args:\n",
      "    query: no wife\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': './A_Dance_With_Dragons.pdf', 'file_path': './A_Dance_With_Dragons.pdf', 'page': 286, 'total_pages': 894, 'format': 'PDF 1.5', 'title': 'A Dance With Dragons - A Song of Ice and Fire', 'author': 'George R R Martin', 'subject': 'nothuman', 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman', 'creator': 'Microsoft® Office Word 2007', 'producer': 'Microsoft® Office Word 2007', 'creationDate': 'D:20110711230404Z', 'modDate': \"D:20190506190144+02'00'\", 'trapped': '', 'start_index': 844}\n",
      "Content: My lord husband will cut your balls off and put you in a dress.” \n",
      "  \n",
      "Qarl rolled off her. “If he can get out of his chair.” \n",
      "  \n",
      "The room was cold. Asha rose from Galbart Glover’s bed and took off her torn clothes. The \n",
      "jerkin would need fresh laces, but her tunic was ruined. I never liked it anyway. She tossed it on the \n",
      "flames. The rest she left in a puddle by the bed. Her breasts were sore, and Qarl’s seed was trickling \n",
      "down her thigh. She would need to brew some moon tea or risk bringing another kraken into the world. \n",
      "What does it matter? My father’s dead, my mother’s dying, my brother’s being flayed, and there’s \n",
      "naught that I can do about any of it. And I’m married. Wedded and bedded … though not by the same \n",
      "man. \n",
      "  \n",
      "When she slipped back beneath the furs, Qarl was asleep. “Now your life is mine. Where did I \n",
      "put my dagger?” Asha pressed herself against his back and slid her arms about him. On the isles he was\n",
      "\n",
      "Source: {'source': './A_Dance_With_Dragons.pdf', 'file_path': './A_Dance_With_Dragons.pdf', 'page': 215, 'total_pages': 894, 'format': 'PDF 1.5', 'title': 'A Dance With Dragons - A Song of Ice and Fire', 'author': 'George R R Martin', 'subject': 'nothuman', 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman', 'creator': 'Microsoft® Office Word 2007', 'producer': 'Microsoft® Office Word 2007', 'creationDate': 'D:20110711230404Z', 'modDate': \"D:20190506190144+02'00'\", 'trapped': '', 'start_index': 1685}\n",
      "Content: hope you will be as loyal to Little Walder when you are joined in wedlock. As to the Starks, that House is \n",
      "extinguished only in the male line. Lord Eddard’s sons are dead, but his daughters live, and the younger \n",
      "girl is coming north to wed brave Ramsay Bolton.” \n",
      "  \n",
      "“Ramsay Snow,” Wylla Manderly threw back. “Have it as you will. By any name, he shall soon be \n",
      "wed to Arya Stark. If you would keep faith with your promise, give him your allegiance, for he shall be \n",
      "your Lord of Winterfell.” \n",
      "  \n",
      "“He won’t ever be my lord! He made Lady Hornwood marry him, then shut her in a dungeon and \n",
      "made her eat her fingers.” \n",
      "  \n",
      "A murmur of assent swept the Merman’s Court. “The maid tells it true,” declared a stocky man \n",
      "in white and purple, whose cloak was fastened with a pair of crossed bronze keys. “Roose Bolton’s cold \n",
      "and cunning, aye, but a man can deal with Roose. We’ve all known worse. But this bastard son of his … \n",
      "they say he’s mad and cruel, a monster.”\n",
      "\n",
      "Source: {'source': './A_Dance_With_Dragons.pdf', 'file_path': './A_Dance_With_Dragons.pdf', 'page': 413, 'total_pages': 894, 'format': 'PDF 1.5', 'title': 'A Dance With Dragons - A Song of Ice and Fire', 'author': 'George R R Martin', 'subject': 'nothuman', 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman', 'creator': 'Microsoft® Office Word 2007', 'producer': 'Microsoft® Office Word 2007', 'creationDate': 'D:20110711230404Z', 'modDate': \"D:20190506190144+02'00'\", 'trapped': '', 'start_index': 895}\n",
      "Content: open, three of the four candles fluttered out. He led the bride into the mist, where the wedding guests \n",
      "were waiting. \n",
      "  \n",
      "“Why me?” he had asked when Lady Dustin told him he must give the bride away. \n",
      "  \n",
      "“Her father is dead and all her brothers. Her mother perished at the Twins. Her uncles are lost or \n",
      "dead or captive.” \n",
      "  \n",
      "“She has a brother still.” She has three brothers still, he might have said. “Jon Snow is with the \n",
      "Night’s Watch.” \n",
      "  \n",
      "“A half-brother, bastard-born, and bound to the Wall. You were her father’s ward, the nearest \n",
      "thing she has to living kin. It is only fitting that you give her hand in marriage.” \n",
      "  \n",
      "The nearest thing she has to living kin. Theon Greyjoy had grown up with Arya Stark. Theon \n",
      "would have known an imposter. If he was seen to accept Bolton’s feigned girl as Arya, the northern lords \n",
      "who had gathered to bear witness to the match would have no grounds to question her legitimacy.\n",
      "\n",
      "Source: {'source': './A_Dance_With_Dragons.pdf', 'file_path': './A_Dance_With_Dragons.pdf', 'page': 470, 'total_pages': 894, 'format': 'PDF 1.5', 'title': 'A Dance With Dragons - A Song of Ice and Fire', 'author': 'George R R Martin', 'subject': 'nothuman', 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman', 'creator': 'Microsoft® Office Word 2007', 'producer': 'Microsoft® Office Word 2007', 'creationDate': 'D:20110711230404Z', 'modDate': \"D:20190506190144+02'00'\", 'trapped': '', 'start_index': 3}\n",
      "Content: “Do you have brothers?” Asha asked her keeper. \n",
      "  \n",
      "“Sisters,” Alysane Mormont replied, gruff as ever. “Five, we were. All girls. Lyanna is back on \n",
      "Bear Island. Lyra and Jory are with our mother. Dacey was murdered.” \n",
      "  \n",
      "“The Red Wedding.” \n",
      "  \n",
      "“Aye.” Alysane stared at Asha for a moment. “I have a son. He’s only two. My daughter’s nine.” \n",
      "  \n",
      "“You started young.” \n",
      "  \n",
      "“Too young. But better that than wait too late.” \n",
      "  \n",
      "A stab at me, Asha thought, but let it be. “You are wed.” \n",
      "  \n",
      "“No. My children were fathered by a bear.” Alysane smiled. Her teeth were crooked, but there \n",
      "was something ingratiating about that smile. “Mormont women are skinchangers. We turn into bears \n",
      "and find mates in the woods. Everyone knows.” \n",
      "  \n",
      "Asha smiled back. “Mormont women are all fighters too.” \n",
      "  \n",
      "The other woman’s smile faded. “What we are is what you made us. On Bear Island every child \n",
      "learns to fear krakens rising from the sea.”\n",
      "\n",
      "Source: {'source': './A_Dance_With_Dragons.pdf', 'file_path': './A_Dance_With_Dragons.pdf', 'page': 667, 'total_pages': 894, 'format': 'PDF 1.5', 'title': 'A Dance With Dragons - A Song of Ice and Fire', 'author': 'George R R Martin', 'subject': 'nothuman', 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman', 'creator': 'Microsoft® Office Word 2007', 'producer': 'Microsoft® Office Word 2007', 'creationDate': 'D:20110711230404Z', 'modDate': \"D:20190506190144+02'00'\", 'trapped': '', 'start_index': 3}\n",
      "Content: Groleo had a wife back in Pentos. Children, grandchildren. Why him, of all the hostages? Jhogo, \n",
      "Hero, and Daario Naharis all commanded fighting men, but Groleo had been an admiral without a fleet. \n",
      "Did they draw straws, or did they think Groleo the least valuable to us, the least likely to provoke \n",
      "reprisal? the knight asked himself … but it was easier to pose that question than to answer it. I have no \n",
      "skill at unraveling such knots. \n",
      "  \n",
      "“Your Grace,” Ser Barristan called out. “If it please you to recall, the noble Yurkhaz died by \n",
      "happenstance. He stumbled on the steps as he tried to flee the dragon and was crushed beneath the \n",
      "feet of his own slaves and companions. That, or his heart burst in terror. He was old.” \n",
      "  \n",
      "“Who is this who speaks without the king’s leave?” asked the Yunkish lord in the striped tokar, a \n",
      "small man with a receding chin and teeth too big for his mouth. He reminded Selmy of a rabbit. “Must\n",
      "\n",
      "Source: {'source': './A_Dance_With_Dragons.pdf', 'file_path': './A_Dance_With_Dragons.pdf', 'page': 413, 'total_pages': 894, 'format': 'PDF 1.5', 'title': 'A Dance With Dragons - A Song of Ice and Fire', 'author': 'George R R Martin', 'subject': 'nothuman', 'keywords': 'A Game of Thrones; A Song of Ice and Fire; pdf, nothuman', 'creator': 'Microsoft® Office Word 2007', 'producer': 'Microsoft® Office Word 2007', 'creationDate': 'D:20110711230404Z', 'modDate': \"D:20190506190144+02'00'\", 'trapped': '', 'start_index': 3}\n",
      "Content: “Help me.” She clutched at him. “Please. I used to watch you in the yard, playing with your \n",
      "swords. You were so handsome.” She squeezed his arm. “If we ran away, I could be your wife, or your … \n",
      "your whore … whatever you wanted. You could be my man.” \n",
      "  \n",
      "Theon wrenched his arm away from her. “I’m no … I’m no one’s man.” A man would help her. \n",
      "“Just … just be Arya, be his wife. Please him, or … just please him, and stop this talk about being \n",
      "someone else.” Jeyne, her name is Jeyne, it rhymes with pain. The music was growing more insistent. “It \n",
      "is time. Wipe those tears from your eyes.” Brown eyes. They should be grey. Someone will see. Someone \n",
      "will remember. “Good. Now smile.” \n",
      "  \n",
      "The girl tried. Her lips, trembling, twitched up and froze, and he could see her teeth. Pretty \n",
      "white teeth, he thought, but if she angers him, they will not be pretty long. When he pushed the door \n",
      "open, three of the four candles fluttered out. He led the bride into the mist, where the wedding guests\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know. The context you provided doesn't mention a wife. It seems to focus on Asha and her relationships, including her husband, Qarl.\n"
     ]
    }
   ],
   "source": [
    "input_message = q_s[2]\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Who drank his way across the narrow sea?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ygritte.\n"
     ]
    }
   ],
   "source": [
    "input_message = q_s[3]\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Create react agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_react_agent(model, [retrieve], checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How many times had Varamyr died?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "According to the book excerpts provided, Varamyr Sixskins had died twice.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"def234\"}}\n",
    "\n",
    "input_message = q_s[6]\n",
    "\n",
    "for event in agent_executor.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "gPduxCNqVGYU"
   },
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EAJE7PHR_9IH",
    "outputId": "73f2f2e1-ae56-4902-ef78-6e8a7b94981f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1796"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del chroma_vectorstore\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZhJBHxjzllO",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Question answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Norm vs Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOu7VD8O05Ef"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_norm(x, gamma, beta, eps=1e-5):\n",
    "  \"\"\"\n",
    "  Performs batch normalization on a given input tensor.\n",
    "\n",
    "  Args:\n",
    "    x: Input tensor of shape (n, d).\n",
    "    gamma: Scale parameter of shape (d,).\n",
    "    beta: Shift parameter of shape (d,).\n",
    "    eps: Small constant for numerical stability.\n",
    "\n",
    "  Returns:\n",
    "    Normalized tensor of the same shape as x.\n",
    "  \"\"\"\n",
    "  mean = np.mean(x, axis=0)\n",
    "  variance = np.var(x, axis=0)\n",
    "  x_hat = (x - mean) / np.sqrt(variance + eps)\n",
    "  return gamma * x_hat + beta\n",
    "\n",
    "\n",
    "def layer_norm(x, gamma, beta, eps=1e-5):\n",
    "  \"\"\"\n",
    "  Performs layer normalization on a given input tensor.\n",
    "\n",
    "  Args:\n",
    "    x: Input tensor of shape (n, d).\n",
    "    gamma: Scale parameter of shape (d,).\n",
    "    beta: Shift parameter of shape (d,).\n",
    "    eps: Small constant for numerical stability.\n",
    "\n",
    "  Returns:\n",
    "    Normalized tensor of the same shape as x.\n",
    "  \"\"\"\n",
    "  mean = np.mean(x, axis=1, keepdims=True)\n",
    "  variance = np.var(x, axis=1, keepdims=True)\n",
    "  x_hat = (x - mean) / np.sqrt(variance + eps)\n",
    "  return gamma * x_hat + beta\n",
    "\n",
    "# Example usage:\n",
    "n = 10\n",
    "d = 5\n",
    "x = np.random.randn(n, d)\n",
    "gamma = np.ones(d)\n",
    "beta = np.zeros(d)\n",
    "\n",
    "x_batch_norm = batch_norm(x, gamma, beta)\n",
    "x_layer_norm = layer_norm(x, gamma, beta)\n",
    "\n",
    "print(\"Batch Normalized x:\")\n",
    "print(x_batch_norm)\n",
    "print(\"\\nLayer Normalized x:\")\n",
    "print(x_layer_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bAg_ADV7wkW"
   },
   "outputs": [],
   "source": [
    "# prompt: Why do transformers use layer norm, instead of batch norm\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"Why do transformers use layer norm, instead of batch norm\"})\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9i3WKv1U4sQn",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### How to choose an embedding model and how to assess its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_E5Feps0xZs"
   },
   "source": [
    "\n",
    "\n",
    " > INSTRUCTOR is the most flexible due to its instruction-tuning capability.\n",
    " >\n",
    " > INSTRUCTOR can be adapted to various specific tasks through instructions.\n",
    " >\n",
    " > If you need highly adaptable embeddings for diverse or specific tasks, choose INSTRUCTOR.\n",
    " >\n",
    " > More flexible as it can be tailored to specific tasks through instructions.\n",
    " >\n",
    " > Generally more powerful but potentially slower than the other options.\n",
    " >\n",
    " > Can handle a wider range of tasks beyond just creating embeddings.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCukHOlA4oRR"
   },
   "source": [
    "### Instruction-tuned model - what is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iKKvuxa42xr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chain of thought prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhances the reasoning capabilities of large language models (LLMs) by incorporating logical steps—or a \"chain of thought\" — within the prompt.\n",
    "\n",
    "CoT guides the model to work through intermediate reasoning steps\n",
    "\n",
    "<img src=\"chain_of_thought_example.png\" style=\"width:800px\">\n",
    "\n",
    "> few-shot prompting (left) and CoT prompting (right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4ffs0Zqm_Fd",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8btdIY15nCPQ"
   },
   "source": [
    " - [Answer relevance](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/)\n",
    "\n",
    " Metric focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy. This metric is computed using the `user_input`, the `retrived_contexts` and the response.\n",
    "\n",
    " The Answer Relevancy is defined as the mean cosine similarity of the original `user_input` to a number of artificial questions, which where generated (reverse engineered) based on the `response`.\n",
    "\n",
    " - [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/factual_correctness/)\n",
    "\n",
    " Faithfulness metric measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better.\n",
    "\n",
    " The generated answer is regarded as faithful if all the claims made in the answer can be inferred from the given context. To calculate this, a set of claims from the generated answer is first identified. Then each of these claims is cross-checked with the given context to determine if it can be inferred from the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4Ta9d9qzp80",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## HNSW\n",
    "\n",
    "Nearest neighbor search. Greedy. Each Vertex is a vector in n-d space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7kHKkKaa9sH"
   },
   "source": [
    "A small-world network is a graph characterized by a high clustering coefficient and low distances.\n",
    "\n",
    "> On an example of social network, high clustering implies the high probability that two friends of one person are friends themselves. The low distances, on the other hand, mean that there is a short chain of social connections between any two people.\n",
    "\n",
    "> Specifically, a small-world network is defined to be a network where the typical distance L between two randomly chosen nodes (the number of steps required) grows proportionally to the logarithm of the number of nodes N in the network, that is:\n",
    ">\n",
    "> $ \\hspace{15mm} L \\propto log N $\n",
    ">\n",
    "> while the global clustering coefficient is not small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBdTbN-cwBj8"
   },
   "source": [
    "### Search in a navigable small world\n",
    "\n",
    " - short range links and long range links - make the graph very navigable\n",
    "\n",
    " - friends list of a vertex\n",
    "\n",
    " - one or a few, predetermined entry points\n",
    "\n",
    " When searching the NSFW graphs, we begin at predetermined entry points. Then we perform Greedy routing. That means out of all the friends, we are goint to navigate to the one closest to our query.\n",
    "\n",
    " If there are no near vertices in friend list, that means it's a local minimum. And this, is a stopping condition.\n",
    "\n",
    " To minimize the probability of stopping early, avoiding local minima, we can increase the average degree of vertices. This also increases the complexity of our network and slows down the search time. We have to find a tradeoff between both of these. Another approach to avoid local minima too early is start search on high degree vertices. The vertices have a lot of collections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUeVkWCRz5wv"
   },
   "source": [
    "A navigable graph, with an entry point. Finding Neighbors of the current point, closest to the query.\n",
    "\n",
    "Greedily explore it in the direction of query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfNzqReKz5rX"
   },
   "source": [
    "### Layers in NSW\n",
    "\n",
    "Take an NSW graph and spread it across multiple layers.\n",
    "\n",
    "High degree vertices will tend to be spread across more layers.\n",
    "\n",
    "When building the graph, we add the number of friends based on which layer it gets inserted at. The higher the layer, that a vertex is inserted at, the more friends it's going to have.\n",
    "\n",
    "If we start at the highest layer, that means we're on a high degree vertex. That means, we are less likely to get stuck on a local minimum and stop early.\n",
    "\n",
    "On each layer, we keep traversing across different edges in that layer, similar to an NSW. We greedily identify and traverse to the friend of the current vertex that has least distance to the query vector, and traverse to it. And we keep doing that.\n",
    "\n",
    "Once we hit the local minimum, we do not still, stop. We move down to the next layer. And we keep doing it until we hit the local minimum at Layer 0 (the bottom-most layer).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbKlbD6UWXZ_"
   },
   "source": [
    "### Building HNSW graph\n",
    "\n",
    " - Insertion\n",
    "\n",
    " - Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqDaD023WcCP"
   },
   "source": [
    "Insertion is guided by a probability function which says, we're going to put a very high number of vertex or vectors on layer 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HdSQzTWzt0k",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbT3heoOGN_5"
   },
   "source": [
    "Stable Diffusion is a generative model used for producing high-quality images from text descriptions. It involves three key components: CLIP, UNet, and VAE. Here's how each component works and their roles in the process:\n",
    "\n",
    "### 1. **CLIP (Contrastive Language-Image Pretraining)**\n",
    "   - **Role**: CLIP acts as the bridge between the text prompt and the image generation process.\n",
    "   - **Function**: CLIP is a model that has been trained to understand the relationship between text and images. It learns to associate text descriptions with images by embedding both into a shared latent space. In Stable Diffusion, CLIP is used to generate an initial understanding of the text prompt, which is then translated into an image during the diffusion process. Specifically, CLIP helps in guiding the image generation process so that the resulting image aligns with the text prompt.\n",
    "\n",
    "### 2. **UNet (U-Net)**\n",
    "   - **Role**: UNet is the core architecture used in the denoising process of the diffusion model.\n",
    "   - **Function**: Stable Diffusion works by iteratively denoising a noisy image. The UNet architecture is designed to take a noisy image as input and predict a less noisy version of that image. This is done over many steps, gradually reducing the noise until a coherent image emerges. The UNet in Stable Diffusion has skip connections, which allow it to capture both global and local features of the image, making it effective in producing high-quality, detailed outputs.\n",
    "\n",
    "### 3. **VAE (Variational Autoencoder)**\n",
    "   - **Role**: VAE is used for compressing and decompressing images during the generation process.\n",
    "   - **Function**: The VAE is used to encode images into a lower-dimensional latent space, which is more manageable for the diffusion process. This latent space captures the essential features of the image, allowing for efficient processing. Once the denoising process is complete, the VAE decoder reconstructs the final high-resolution image from this latent space. The VAE helps to ensure that the generated images are realistic and adhere to the underlying distribution of the training data.\n",
    "\n",
    "### **How It Works Together:**\n",
    "1. **Text to Latent Space**: The process begins with a text prompt, which is processed by the CLIP model to generate a text embedding.\n",
    "2. **Latent Diffusion Process**: An initial noisy image is generated in the latent space, corresponding to the text embedding. The UNet model is then used to iteratively denoise this image.\n",
    "3. **Decoding**: After sufficient denoising, the latent image is passed through the VAE decoder, which converts it back into a high-resolution image in pixel space.\n",
    "\n",
    "In summary, Stable Diffusion leverages CLIP to understand and align text with image generation, UNet to iteratively refine the image, and VAE to manage the latent space for efficient image generation. The combination of these components allows Stable Diffusion to generate high-quality images that closely match the given text descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6GdBKLSGtP6"
   },
   "source": [
    "> ### It learns to associate text descriptions with images by embedding both into a shared latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNAHbe90GzDx"
   },
   "source": [
    "Certainly! Let's dive deeper into how CLIP associates text descriptions with images through the concept of embedding into a shared latent space.\n",
    "\n",
    "### **1. Embedding in CLIP:**\n",
    "\n",
    "CLIP, developed by OpenAI, is a model that can understand both text and images. It achieves this understanding by converting (or embedding) both text and images into vectors in a shared latent space—a high-dimensional mathematical space where similar items are close together, and dissimilar items are far apart.\n",
    "\n",
    "#### **a. Text Embedding:**\n",
    "- **Process**: CLIP takes a text input (like a sentence) and processes it through a text encoder, often based on a Transformer architecture. This encoder transforms the text into a fixed-size vector, capturing the semantic meaning of the text.\n",
    "- **Output**: The result is a vector representation of the text, where each dimension in the vector captures some aspect of the text's meaning. This vector is located in the latent space where the model has learned to place textual concepts.\n",
    "\n",
    "#### **b. Image Embedding:**\n",
    "- **Process**: Similarly, CLIP takes an image and processes it through an image encoder, often a convolutional neural network (CNN). This encoder extracts features from the image (like colors, shapes, textures) and converts them into a vector.\n",
    "- **Output**: This vector is a representation of the image, capturing its essential features. Just like the text vector, this image vector exists in the same latent space.\n",
    "\n",
    "### **2. Shared Latent Space:**\n",
    "The \"shared latent space\" is a conceptual space where both text and image vectors are placed after being processed by their respective encoders. The goal during training is to make sure that a text and an image that match (e.g., \"a cat\" and a picture of a cat) end up being close together in this space, while unrelated text and images (e.g., \"a cat\" and a picture of a car) are far apart.\n",
    "\n",
    "### **3. Learning to Associate:**\n",
    "CLIP is trained on a massive dataset of text-image pairs (like captions paired with images). During training:\n",
    "- **Alignment**: The model learns to adjust the embeddings so that the vectors for matching text and images are closer together, and non-matching pairs are further apart.\n",
    "- **Loss Function**: CLIP uses a contrastive loss function during training, which explicitly encourages this proximity for matching pairs and separation for non-matching pairs.\n",
    "\n",
    "### **4. Using CLIP in Stable Diffusion:**\n",
    "In the context of Stable Diffusion:\n",
    "- When you input a text prompt, CLIP generates a vector representing that text in the latent space.\n",
    "- This vector guides the diffusion model (which generates the image) by ensuring that the generated image's latent representation stays close to the text's latent representation, ensuring that the image aligns with the given description.\n",
    "\n",
    "### **Why This Matters:**\n",
    "By embedding both text and images into the same latent space, CLIP enables models like Stable Diffusion to understand the relationship between text and image. This shared understanding is crucial for generating images that accurately reflect the text prompts provided by users.\n",
    "\n",
    "In summary, the shared latent space in CLIP allows for a powerful association between text and images, enabling models to generate images that are semantically aligned with textual descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0B429r9GREZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Qo2ekmK9pliQ",
    "Lcpo7tLoMfws",
    "ubzj_lSgpoUL",
    "0uxINd7Zg--f",
    "HUoxa5BtMW3r",
    "tFLIPqEu-AFt",
    "gZhJBHxjzllO",
    "O4Ta9d9qzp80",
    "8HdSQzTWzt0k"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
